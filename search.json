[
  {
    "objectID": "vignettes.html",
    "href": "vignettes.html",
    "title": "R Vignettes",
    "section": "",
    "text": "This page consists of some Vignettes I have developed utilizing the R programming language.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Relevance Determination for Functional Data Representation\n\n\n…a Vignette on Functional Data Analysis\n\n\nAn approach to functional data representation based on automatic relevance determination.\n\n\n\n\n\nJul 10, 2025\n\n\nFelipe Toledo Ferreira\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "vignettes/ardRepresentation.html",
    "href": "vignettes/ardRepresentation.html",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "",
    "text": "Last updated 2025/07/10."
  },
  {
    "objectID": "vignettes/ardRepresentation.html#data",
    "href": "vignettes/ardRepresentation.html#data",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Data",
    "text": "Data\nFor this particular study, we consider two distinct data sets, the Motorcycle data set and the Canadian weather data set: the former is frequently studied in the overall spline smoothing context (as well as overall studies in Nonparametric Statistics), whilst the latter is most often used in the specific context of Functional Data Analysis. We will also present a brief simulation study. The R chunk provided below loads the relevant packages utilized throughout.\n\n\nShow R code\nlibrary(\"tidyverse\")  ## R package for tidy data manipulation and visualization\nlibrary(\"extrafont\")  ## R package for using nice fonts in figures\nlibrary(\"fda\")        ## R package for functional data analysis\nlibrary(\"MASS\")\nlibrary(\"magrittr\")   ## R package for pipes\nlibrary(\"patchwork\")  ## R package for plot composition\nlibrary(\"reshape2\")\nlibrary(\"rstan\")\n\n\n\nThe Motorcycle data set\nThe Motorcycle data set (seen in Silverman (1985), and available in the R programming language in the package MASS) consists of \\(n = 133\\) measurements obtained through an experiment which simulated motorcycle accidents with the goal of determining the efficacy of crash helmets: an accelerometer was fitted to an object which was later submitted to repeated mechanical impacts, and the resulting acceleration was measured in the instants subsequent to the impacts. We denote the measurements of the time (in milliseconds) elapsed since the impact occurs as \\(t_j\\), and the acceleration (in g) measured at the corresponding instant as \\(y_j\\), hence we denote the full data set as \\(\\{t_j, y_j\\}^n_{j = 1}\\). Some observations can be made with respect to the set of instants \\(\\{t_j\\}^n_{j = 1}\\): as the data set was gathered through repeat experiments, the measured instants are not equally spaced, and some appear multiple times (with distinct corresponding \\(y_j\\)). Let \\(Y_j\\) denote a random variable corresponding to the \\(j\\)-th accelerometer measurement, we assume that\n\\[\n  \\mathbb{E}[Y_j] = g(t_j) \\quad j \\in \\{1,\\ldots,n\\}.\n\\tag{1}\\]\nThat is, we assume that albeit the accelerometer measurements are randomly distributed, they present some regularity in the form of a mean, which is dependent on the instant at which the measurement is made, expressed as the mean function \\(g(\\cdot)\\)1. In particular, we’ll assume that \\(Y_j \\sim \\text{Normal}(g(t_j), 1/\\tau)\\), \\(\\tau &gt; 0\\), or equivalently\n\\[\n  y_j = g(t_j) + \\tau^{-1/2}\\varepsilon_j \\quad j \\in \\{1,\\ldots,n\\},\n\\tag{2}\\]\nwhere \\(\\varepsilon_j \\sim \\text{Normal}(0,1), j \\in \\{1,\\ldots,n\\}\\)2. The first term on the r.h.s. of Equation 2 is the ‘deterministic component’, whilst the second term on the r.h.s. of Equation 2 is the ‘stochastic component’. Our goal is to construct an estimator \\(\\hat{g}(\\cdot)\\) for the deterministic component. Naturally, searching for an optimal \\(\\hat{g}(\\cdot) \\in \\mathcal{G}\\), where \\(\\mathcal{G}\\) is an arbitrary functional space (e.g. the space of square integrable functions) is computationally unfeasible, as \\(\\mathcal{G}\\) would likely be infinite-dimensional. For that purpose, first we will assume that \\(g(\\cdot)\\) may be written as the linear combination of a set of basis functions \\(\\{\\phi_k(\\cdot)\\}_{k = 1}^{K}\\) of the following form\n\\[\n  g(t) = \\sum^K_{k = 1}\\beta_{k} \\phi_k(t),\n\\tag{3}\\]\nfor some \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^{K}\\). This allows us to recontextualize the usually infinite-dimensional problem of estimating \\({g}(\\cdot) \\in \\mathcal{G}\\) as a finite-dimensional problem of estimating \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^K\\). Ideally, the set of basis functions \\(\\{\\phi_k(\\cdot)\\}_{k = 1}^K\\) would be chosen so as to both reflect properties we believe the function \\(g(\\cdot)\\) possesses (e.g., continuity to the \\(M\\)-th order) and to provide a sufficiently flexible foundation which may approximate several distinct forms of \\(g(\\cdot)\\) (there are also data-driven choices for basis functions, such as via functional principal components analysis). A natural choice for a set of basis functions are those generated in the B-spline framework (see De Boor (1978)). In particular, we’ll consider cubic B-splines with equidistant knots (with the first knot defined as \\(\\min_j\\{t_j\\}\\) and the last knot as \\(\\max_j\\{t_j\\}\\)), such that the number of basis functions in a basis set is \\(K\\) (including boundary B-splines), with \\(K \\in \\{5,10,15,20,25,30\\}\\). Let \\(\\boldsymbol{\\Phi} \\in \\mathbb{R}^{K \\times K}\\) and \\(\\boldsymbol{\\phi}(t) \\in \\mathbb{R}^K\\) be defined as \\[\n  \\boldsymbol{\\Phi} = \\begin{pmatrix}\n    \\phi_{1}(t_1) & \\phi_{2}(t_1) & \\dots & \\phi_{K}(t_1) \\\\\n    \\phi_{1}(t_2) & \\phi_{2}(t_2) & \\dots & \\phi_{K}(t_2) \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\phi_{1}(t_n) & \\phi_{2}(t_n) & \\dots & \\phi_{K}(t_n)\n  \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{\\phi}(t) = \\begin{pmatrix}\n    \\phi_1(t) \\\\\n    \\phi_2(t) \\\\\n    \\vdots \\\\\n    \\phi_K(t)\n  \\end{pmatrix},\n\\tag{4}\\] we determine the estimates for the coefficients \\(\\boldsymbol{\\beta}\\) via maximum likelihood estimation as \\[\n  \\begin{aligned}\n    \\hat{\\boldsymbol{\\beta}}_\\text{ML} & = \\arg \\max_{\\boldsymbol{\\beta}\\in\\mathbb{R}^K} \\Bigg\\{-\\frac{n}{2}\\ln(2 \\pi) + \\frac{n}{2} \\ln(\\tau) -\\frac{\\tau}{2}\\sum^n_{j = 1}(y_j - \\boldsymbol{\\beta}^\\top \\boldsymbol{\\phi}(t_j))^2 \\Bigg\\} \\\\\n    & = \\arg \\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^K} \\Bigg\\{\\sum^n_{j = 1}(y_j - \\boldsymbol{\\beta}^\\top \\boldsymbol{\\phi}(t_j))^2\\Bigg\\} \\\\\n    \\hat{\\boldsymbol{\\beta}}_\\text{ML} & = (\\boldsymbol{\\Phi}^\\text{T} \\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^\\text{T}\\mathrm{y},\n  \\end{aligned}\n\\] assuming \\(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\) is positive-definite. Therefore, the estimated mean function \\(\\hat{g}(\\cdot)\\) evaluated at a point \\(s\\) is defined as \\[\n  \\hat{g}(s) = \\boldsymbol{\\hat{\\beta}}^\\text{T}_{\\text{ML}} \\boldsymbol{\\phi}(s).\n\\] Figure 1 illustrates the estimated mean functions \\(\\hat{g}(\\cdot)\\) obtained via maximum likelihood for each \\(K\\).\n\n\nShow R code\nset.seed(747)\nK = c(5, 10, 15, 20, 25, 30)\neval_x = mcycle %&gt;% {seq(from = min(.$times), to = max(.$times), length.out = 1e3 + 1)}\nmcycle_fda = \n  lapply(X = seq_along(K),\n         FUN = function(x){\n           design_basis = create.bspline.basis(rangeval = range(mcycle$times),\n                                               norder = 4,\n                                               nbasis = K[x])\n           design_matrix = design_basis %&gt;%\n             getbasismatrix(evalarg = mcycle$times,\n                            basisobj = .,\n                            nderiv = 0)\n           design_matrixf = design_basis %&gt;%\n             getbasismatrix(evalarg = eval_x,\n                            basisobj = .,\n                            nderiv = 0)\n           coefficients = design_matrix %&gt;%\n             {solve(a = t(.) %*% .,\n                    b = t(.) %*% mcycle$accel)}\n           return(list(K = K[x],\n                       design_matrix = design_matrix,\n                       design_matrixf = design_matrixf,\n                       coefficients = coefficients,\n                       fitted_values = design_matrix %*% coefficients,\n                       eval_values = design_matrixf %*% coefficients))\n           })\n(lapply(X = seq_along(K),\n       FUN = function(x){\n         data.frame(values = mcycle_fda[[x]]$eval_values,\n                    eval_x = eval_x,\n                    x = factor(K[x],\n                               levels = K))\n       }) %&gt;% {do.call(what = rbind,\n                       args = .)} %&gt;%\n  ggplot(data = .,\n         mapping = aes(x = eval_x,\n                       y = values,\n                       group = x,\n                       col = x)) +\n  geom_line() +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125),\n        legend.position = \"bottom\") +\n  labs(x = substitute(paste(\"Time after impact (in \", italic(\"ms\"), \")\")),\n       y = substitute(paste(\"Acceleration (in \", italic(\"g\"), \")\")),\n       col = substitute(italic(K))) +\n  scale_color_discrete(type = viridis::viridis(n = 6)) +\n    geom_hline(col = \"red\",\n               linetype = \"dashed\",\n               yintercept = 0) +\n  (mcycle |&gt; geom_point(inherit.aes = F,\n                        mapping = aes(x = times,\n                                      y = accel)))) / guide_area() + plot_layout(heights = c(4, .5))\n\n\n\n\n\n\n\n\nFigure 1: Estimated mean functions for the Motorcycle data set, for \\(K \\in \\{5,10,15,20,25,30\\}\\). The black dots represent the sampled data, and the dashed red line is represents to \\(y = 0\\).\n\n\n\n\n\nSome remarks can be made with respect to the results of Figure 1: first, we note that for small \\(K\\) (say \\(K \\in \\{5,10\\}\\)), the estimated mean function \\(\\hat{g}(\\cdot)\\) underfits the data, and is unable to capture localized trends. By contrast, for large \\(K\\) (say \\(K \\in \\{25,30\\}\\)), the estimated mean function overfits the data, extrapolating trends which are potentially spurious. This behavior is expected, and is well known in Statistics as the ‘bias-variance tradeoff’. A more interesting remark can be made after we observe Figure 2, which illustrates the estimated coefficient values \\(\\{\\hat{\\boldsymbol{\\beta}}_\\text{ML}\\}_k\\) for each \\(K\\).\n\n\nShow R code\ncoef_plots =\n  lapply(X = seq_along(K),\n         FUN = function(x){\n           g = mcycle_fda[[x]] %&gt;%\n             {data.frame(x = seq_along(.$coefficients),\n                         y = .$coefficients) |&gt;\n                 ggplot(mapping = aes(x = x,\n                                      y = y)) +\n                 geom_line(col = \"black\",\n                           linetype = \"dashed\") +\n                 geom_point() +\n                 geom_hline(yintercept = 0,\n                            linetype = \"dashed\",\n                            col = \"red\") +\n                 scale_x_continuous(breaks = c(1, seq(from = 5, to = K[x], by = 5))) +\n                 labs(x = substitute(paste(\"Basis index (\", italic(k), \")\")),\n                      y = substitute(paste(\"Estimated coefficient (\", hat(beta)[k], \")\")))}\n           return(list(g = g))\n         })\ncoef_plots[[1]]$g /\ncoef_plots[[2]]$g /\ncoef_plots[[3]]$g /\ncoef_plots[[4]]$g /\ncoef_plots[[5]]$g /\ncoef_plots[[6]]$g +\n  plot_layout(axes = 'collect',\n              guides = 'collect') &\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125))\n\n\n\n\n\n\n\n\nFigure 2: Estimated coefficients for basis expansions of the Motorcycle data set, with \\(K \\in \\{5,10,15,20,25,30\\}.\\)\n\n\n\n\n\nIt is apparent in Figure 2, particularly for larger \\(K\\), that some coefficients are very close to zero (though they are not zero). Particularly, this occurs to coefficients corresponding to basis functions which model the start of the mean function (around 15ms after impact), and a few of those which model the latter half of the mean function (around 40ms after impact). This yields mean function estimates close to zero (as can be seen in Figure 1)3. Though the coefficient estimates may be small, and likewise do not possess statistical or practical significance, they are nevertheless not zero (except in essentially pathological cases). As such, our model would include all \\(K\\) basis functions. A more efficient estimator would automatically identify basis functions whose significance is below a certain threshold and prune those out of the model, yielding a model with only \\(K_0 \\leq K\\) non-zero coefficients, with a comparative performance to the full model. Some procedures for automatic variable selection have been developed, which include the Least Absolute Shrinkage and Selection Operator (LASSO; see Tibshirani (1996)), the Smoothly Clipped Absolute Deviation (SCAD; see Fan and Li (2001)), the Minimax Concave Penalty (MCP; see Zhang (2010)), the Horseshoe prior (see Carvalho, Polson, and Scott (2010)) and Automatic Relevance Determination (ARD; see Tipping (2001)). In Section 2.1 of this vignette we illustrate the application of the ARD procedure to the Motorcyle data set.\n\n\nThe Canadian weather data set\nThe Canadian weather data set (seen in J. O. Ramsay and Silverman (2002), and available in the fda package in R) consists of measurements obtained across \\(N = 35\\) Canadian cities: in particular, we focus on the daily average temperatures for each of these cities, averaged from 1960-1994 (hence, for every city we have \\(n = 365\\) temperature measurements). We denote the day of the measurement as \\(t_{ij} \\in \\{1,2,\\ldots,365\\}\\), and the daily average temperature (in Celsius) as \\(y_{ij}\\), where the subscript \\(i \\in \\{1,2,\\ldots,35\\}\\) indicates the city to which the measurement corresponds, and the subscript \\(j \\in \\{1,2,\\ldots,365\\}\\) indicates the day to which the measurement corresponds. Therefore, we denote the full data set as \\(\\{t_{ij},y_{ij}\\}^{N,n}_{i = 1,j = 1}\\)4. Similarly to the Motorcycle data set context, we denote \\(Y_{ij}\\) as the random variable corresponding to the average temperature of the \\(j\\)-th day on the \\(i\\)-th city, and assume that\n\\[\n  \\mathbb{E}[Y_{ij}] = g_i(t_{ij}) \\quad i \\in \\{1,\\ldots,N\\}, j \\in \\{1,\\ldots,n\\}.\n\\] A key difference between this formulation and that which is seen in Equation 1 is that the mean function is allowed to vary dependent on the city, that is, we assume that the mean daily temperature profile is different in each city5. Again, we’ll assume that \\(Y_{ij} \\sim \\text{Normal}(g(t_{ij}), 1/\\tau_i), \\tau_i &gt; 0\\), or equivalently\n\\[\n  y_{ij} = g_i(t_{ij}) + \\tau^{-1/2}_i \\varepsilon_{ij} \\quad i \\in \\{1,\\ldots,N\\}, j \\in \\{1,\\ldots,n\\},\n\\] where \\(\\varepsilon_{ij} \\sim \\text{Normal}(0,1), i \\in \\{1,\\ldots,N\\}, j \\in \\{1,\\ldots,n\\}\\). Again we seek to determine estimators \\(\\hat{g}_i(\\cdot)\\) for the mean temperature functions, and likewise we assume that the mean temperature functions may be decomposed as linear combinations of a set of basis functions \\(\\{\\phi_k(\\cdot)\\}_{k = 1}^K\\) of the form \\[\n  g_i(t) = \\sum^K_{k = 1}\\beta_{ik} \\phi_k(t),\n\\] for some \\(\\boldsymbol{\\beta}_i \\in \\mathbb{R}^K\\). Unlike the Motorcycle data set application, we consider a different set of basis functions: as we know that the observed functions represent weather observations, we find it ideal that the basis functions accommodate periodic trends that may appear. One ideal family of basis functions in such case is the family of Fourier basis functions (in particular, we adopt \\(K = 51\\) Fourier basis functions - also including an intercept term - with a periodicity of \\(365\\), starting at \\(t = 1/2\\) and ending at \\(t = 365 + 1/2\\), where the additional \\(1/2\\) serves to accommodate the transition between the last day of one year and the first day of the next). Disadvantageous to the Fourier approximation, however, is that the Fourier design matrix is not sparse, as the B-spline design matrix may be, and consequently the usage of Fourier basis is more computationally costly. Let \\(\\boldsymbol{\\Phi}_i \\in \\mathbb{R}^{K \\times K}\\) and \\(\\boldsymbol{\\phi}_i(t) \\in \\mathbb{R}^K\\) be defined as \\[\n  \\boldsymbol{\\Phi}_i = \\begin{pmatrix}\n    \\phi_{1}(t_{i1}) & \\phi_{2}(t_{i1}) & \\dots & \\phi_{K}(t_{i1}) \\\\\n    \\phi_{1}(t_{i2}) & \\phi_{2}(t_{i2}) & \\dots & \\phi_{K}(t_{i2}) \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\phi_{1}(t_{in}) & \\phi_{2}(t_{in}) & \\dots & \\phi_{K}(t_{in})\n  \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{\\phi}_i(t) = \\begin{pmatrix}\n    \\phi_1(t) \\\\\n    \\phi_2(t) \\\\\n    \\vdots \\\\\n    \\phi_K(t)\n  \\end{pmatrix}.\n\\] Analogously to previous results, we find that the maximum likelihood estimators of the coefficients \\(\\boldsymbol{\\beta}_i\\) are \\[\n  \\{\\hat{\\boldsymbol{\\beta}}_\\text{ML}\\}_i = (\\boldsymbol{\\Phi}_i^\\text{T}\\boldsymbol{\\Phi}_i)^{-1}\\boldsymbol{\\Phi}^\\text{T}_i\\mathrm{y}_i \\quad i \\in \\{1,\\ldots,N\\},\n\\] assuming \\(\\boldsymbol{\\Phi}_i^\\text{T} \\boldsymbol{\\Phi}_i\\) is positive-definite for \\(i \\in \\{1,\\ldots,N\\}\\). The estimated mean functions, as well as the original observed data, are plotted in Figure 3.\n\n\nShow R code\nK_canadian = 51\neval_x_canadian = seq(from = .5, to = 365.5, length.out = 1e3 + 1)\ncanadianWeather_fda =\n  lapply(X = seq_along(CanadianWeather$place),\n         FUN = function(x){\n           design_basis = create.fourier.basis(rangeval = c(.5, 365.5),\n                                               nbasis = K_canadian,\n                                               period = 365)\n           design_matrix = design_basis %&gt;%\n             getbasismatrix(evalarg = 1:365,\n                            basisobj = .,\n                            nderiv = 0)\n           design_matrixf = design_basis %&gt;%\n             getbasismatrix(evalarg = eval_x_canadian,\n                            basisobj = .,\n                            nderiv = 0)\n           coefficients = design_matrix %&gt;%\n             {solve(a = t(.) %*% .,\n                    b = t(.) %*% CanadianWeather$dailyAv[ , x , 1])}\n           return(list(K = K_canadian,\n                       design_matrix = design_matrix,\n                       design_matrixf = design_matrixf,\n                       coefficients = coefficients,\n                       fitted_values = design_matrix %*% coefficients,\n                       eval_values = design_matrixf %*% coefficients,\n                       place = CanadianWeather$place[x],\n                       day = 1:365))\n           })\n(lapply(X = seq_along(CanadianWeather$place),\n       FUN = function(x){\n         data.frame(values = canadianWeather_fda[[x]]$eval_values,\n                    eval_x = eval_x_canadian,\n                    x = factor(CanadianWeather$place[x],\n                               levels = unique(CanadianWeather$place)),\n                     region = factor(CanadianWeather$region[x],\n                               levels = unique(CanadianWeather$region)),\n                     row.names = NULL)\n       }) %&gt;% {do.call(what = rbind,\n                       args = .)} %&gt;%\n  ggplot(data = .,\n         mapping = aes(x = eval_x,\n                       y = values,\n                       group = x,\n                       col = region)) +\n  geom_line() +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125),\n        legend.position = \"bottom\") +\n  labs(x = \"Day of the year\",\n       y = substitute(paste(\"Temperature (in \", italic(\"°C\"), \")\")),\n       col = \"Region\") +\n  scale_color_discrete(type = viridis::viridis(n = 4)) +\n    scale_x_continuous(breaks = c(1, 100, 200, 300, 365)) +\n    geom_hline(yintercept = 0,\n                linetype = \"dashed\",\n                col = \"red\")) +\n  (lapply(X = seq_along(CanadianWeather$place),\n       FUN = function(x){\n         data.frame(values = CanadianWeather$dailyAv[ , x , 1],\n                    eval_x = 1:365,\n                    x = factor(CanadianWeather$place[x],\n                               levels = unique(CanadianWeather$place)),\n                     region = factor(CanadianWeather$region[x],\n                               levels = unique(CanadianWeather$region)))\n       }) %&gt;% {do.call(what = rbind,\n                       args = .)} %&gt;%\n  geom_line(inherit.aes = F,\n            data = .,\n            mapping = aes(x = eval_x,\n                          y = values,\n                          group = x,\n                          col = region)))\n\n\n\n\n\n\n\n\nFigure 3: Estimated mean functions for each city in the Canadian weather data set, with \\(K = 51\\). The original observed data are likewise plotted.\n\n\n\n\n\nAs we deal with a larger number of functions to estimate in this application, we opt to attribute to all expansions the same number of basis functions (\\(K = 51\\)). As may be seen in Figure 3, the original data set presents a variable region which divides the cities with respect to their corresponding climate zones, although this characteristic is not further explored in this vignette.\n\n\nSimulated data\nIn addition to the two data sets presented above, we also consider two distinct scenarios of simulated data. Firstly, we consider a data set comprised of \\(N\\) functions sampled \\(n\\) times each, generated from a cubic B-spline basis set of functions, with equidistant knots starting at \\(0\\) and ending at \\(1\\), such that we have 10 basis functions, however only 6 have non-zero true coefficients. The sampling design is regular (see the footnotes) in the interval \\([0,1]\\). Our goal with this simulation is to determine the accuracy of the estimated coefficients for the ARD procedure. The data is generated as follows\n\\[\n  y_{ij} = \\boldsymbol{\\phi}^\\text{T}(t_{ij})\\begin{pmatrix}\n    -2 \\\\\n    0 \\\\\n    3/2 \\\\\n    3/2 \\\\\n    0 \\\\\n    -1 \\\\\n    -1/2 \\\\\n    -1 \\\\\n    0 \\\\\n    0\n  \\end{pmatrix} + \\tau^{-1/2}\\varepsilon_{ij},\n\\tag{5}\\] where \\(\\boldsymbol{\\phi}(t)\\) is as in Equation 4 and \\(\\tau = 25\\). Figure 4 illustrates one generated replication of this data set.\n\n\nShow R code\nset.seed(23121998)\nreplication_1 = 1000\nN_1 = 10\nn_1 = 10\nt_1 = seq(from = 0, to = 1, length.out = n_1)\ntau_1 = .2 ** -2\nbeta_1 = c(-2, 0, 3/2, 3/2, 0, -1, -1/2, -1, 0, 0)\nK_1 = length(beta_1)\nbeta_in_1 = which(beta_1 != 0)\nbeta_out_1 = which(beta_1 == 0)\nPhi_1 = create.bspline.basis(rangeval = c(0, 1),\n                             norder = 4,\n                             nbasis = K_1) %&gt;%\n  getbasismatrix(evalarg = t_1,\n                 basisobj = .,\n                 nderiv = 0)\nmean_1 = Phi_1 %*% beta_1\nsim_data_1 = lapply(X = 1:N_1,\n                    FUN = function(x){\n                      y = mean_1 + (tau_1 ** (-1/2)) * rnorm(n = n_1)\n                      return(y)\n                    })\n(lapply(X = 1:N_1,\n        FUN = function(x){\n          data.frame(x = t_1,\n                     y = sim_data_1[[x]],\n                     ind = factor(x, levels = 1:N_1))\n        })) %&gt;% do.call(what = rbind,\n                        args = .) |&gt;\n  ggplot(mapping = aes(x = x,\n                       y = y,\n                       group = ind,\n                       col = ind)) +\n  geom_line() +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125),\n        legend.position = \"none\") +\n  labs(x = substitute(paste(italic(t)[ij])),\n       y = substitute(paste(italic(y)[ij]))) +\n  scale_color_discrete(type = viridis::viridis(n = N_1)) +\n    scale_x_continuous(breaks = c(0, .5, 1),\n                       labels = c(\"0\", \"1/2\", \"1\")) +\n    geom_hline(yintercept = 0,\n                linetype = \"dashed\",\n                col = \"red\")\n\n\n\n\n\n\n\n\nFigure 4: \\(N = 10\\) functions sampled at \\(n = 10\\) instants according to Equation 5\n\n\n\n\n\nFor the second simulation, we generated a data set composed of \\(N\\) functions sampled \\(n\\) times each (again, the sampling design is regular in the interval \\([0,2\\pi]\\)), where the mean curve is the sum of two trigonometric functions, as in\n\\[\n  y_{ij} = \\cos(t_{ij}) + \\sin(2 t_{ij}) + \\tau^{-1/2}\\varepsilon_{ij},\n\\tag{6}\\]\nwith \\(\\tau = 25\\). Unlike the previous simulation, we consider the mean function expansions \\(\\hat{g}_i(\\cdot)\\) as resulting from a linear combination of \\(K = 10\\) Fourier basis functions, with periodicity equal to \\(2\\pi\\), starting at \\(0\\). Similarly to the previous simulation, this implies that the coefficients in the resulting expansion must be non-zero for only two of the basis functions. Figure 5 illustrates one generated replication of this data set.\n\n\nShow R code\nset.seed(12231998)\nreplication_2 = 1000\nN_2 = 10\nn_2 = 10\nt_2 = seq(from = 0, to = 2 * pi, length.out = n_2)\ntau_2 = .2 ** -2\nbeta_2 = c(0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0)\nK_2 = length(beta_2)\nbeta_in_2 = which(beta_2 != 0)\nbeta_out_2 = which(beta_2 == 0)\nmean_2 = cos(t_2) + sin(2 * t_2)\nsim_data_2 = lapply(X = 1:N_2,\n                    FUN = function(x){\n                      y = mean_2 + (tau_2 ** (-1/2)) * rnorm(n = n_2)\n                      return(y)\n                    })\n(lapply(X = 1:N_2,\n        FUN = function(x){\n          data.frame(x = t_2,\n                     y = sim_data_2[[x]],\n                     ind = factor(x, levels = 1:N_2))\n        })) %&gt;% do.call(what = rbind,\n                        args = .) |&gt;\n  ggplot(mapping = aes(x = x,\n                       y = y,\n                       group = ind,\n                       col = ind)) +\n  geom_line() +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125),\n        legend.position = \"none\") +\n  labs(x = substitute(paste(italic(t)[ij])),\n       y = substitute(paste(italic(y)[ij]))) +\n  scale_color_discrete(type = viridis::viridis(n = N_1)) +\n    scale_x_continuous(breaks = c(0, pi, 2 * pi),\n                       labels = expression(\"0\", pi, paste(\"2\", pi))) +\n    geom_hline(yintercept = 0,\n                linetype = \"dashed\",\n                col = \"red\")\n\n\n\n\n\n\n\n\nFigure 5: \\(N = 10\\) functions sampled at \\(n = 10\\) instants according to Equation 6\n\n\n\n\n\nFor both simulations we’ll consider 9 possible scenarios, with varying values for \\(N\\) and \\(n\\), whice are presented in Table 1.\n\n\n\n\n\n\n\n\nScenario\nN\nn\n\n\n\n\n1\n10\n10\n\n\n2\n10\n100\n\n\n3\n10\n1000\n\n\n4\n100\n10\n\n\n5\n100\n100\n\n\n6\n100\n1000\n\n\n7\n1000\n10\n\n\n8\n1000\n100\n\n\n9\n1000\n1000\n\n\n\n\n\n\nTable 1: Scenarios for the simulation studies."
  },
  {
    "objectID": "vignettes/ardRepresentation.html#setup",
    "href": "vignettes/ardRepresentation.html#setup",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Setup",
    "text": "Setup\nThis vignette is primarily concerned with presenting an approach to sparse representation of Functional Data utilizing the Automatic Relevance Determination (ARD) framework, itself only a topic in the wider Sparse Bayesian Learning (SBL) context. More precisely, our aim herein is to replicate some of the studies in Sousa, Souza, and Dias (2024) and Cruz, Souza, and Sousa (2024) (the former moreso than the latter, as we do not account for any correlation structure in the functions). The goal of these previous studies was to provide an adaptive Bayesian procedure to select the bases for functional data representation, the latter also extending the results of the former by including a correlation structure for the functional data, and delineating an estimation procedure utilizing the Variational Bayes (VB) approach. It did so by including a collection of Bernoulli random variables (denoted \\(Z_{k}\\)), each associated with one particular term in the basis expansion in Equation 3, resulting in the form \\(\\tilde{\\beta}_{k} = Z_{k} \\beta_{k}\\), such that \\(\\mathbb{P}(\\tilde{\\beta}_{k} = 0) &gt; 0\\). The ARD approach, which is presented subsequently, similarly works as Bayesian model.\n\nAutomatic Relevance Determination\nThe ARD approach attributes the following hierarchical Bayesian model:\n\\[\n  \\begin{aligned}\n    y(t_{j}) \\vert \\{\\beta_{k}\\}, \\tau & \\sim \\text{Normal}\\Bigg(\\sum^K_{k = 1}\\beta_{k}\\phi_k(t_{j}), \\tau^{-1}\\Bigg) & j \\in \\{1,\\ldots,n\\} \\\\\n    {\\beta}_{k} \\vert  \\alpha_{k} & \\sim \\text{Normal}(0,\\alpha_{k}^{-1}) & k \\in \\{1,\\ldots,K\\}.\n  \\end{aligned}\n\\] Where \\(\\{\\alpha_{k}\\}\\) (greater than zero) are hyperparameters which dictate the concentration of the coefficients around zero, whilst \\(\\tau\\) (also greater than zero) is a hyperparameter which dictates the precision of the sampled observations. In adopting the empirical Bayes approach to estimate the model, we aim to determine the hyperparameter values which maximize the distribution of the observations points marginalized over the coefficients \\(\\{\\beta_{k}\\}\\). Tipping (2001) proposes a straightforward set of update equations for the hyperparameters (as well as for the coefficients, which are estimated via their maximum a posteriori value). These are as follows:\n\\[\n  \\hat{\\boldsymbol{\\beta}} = \\hat{\\tau} \\boldsymbol{\\Sigma}\\Phi^\\text{T}\\mathrm{y}\n\\tag{7}\\]\n\\[\n  \\hat{\\alpha}_k = \\frac{\\gamma_k}{\\hat{\\beta}_k^2}\n\\tag{8}\\]\n\\[\n  \\frac{1}{\\hat{\\tau}} = \\frac{\\lvert\\lvert \\textbf{y} - \\Phi \\hat{\\boldsymbol{\\beta}}\\rvert\\rvert^2}{n - \\sum^K_{k = 1}\\gamma_{k}} \\quad k \\in \\{1,\\ldots,K\\}\n\\tag{9}\\]\nwhere\n\\[\n  \\boldsymbol{\\Sigma} = (\\text{diag}(\\hat{\\alpha}) + \\hat{\\tau} \\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi})^{-1}\n\\tag{10}\\]\n\\[\n  \\gamma_{k} = 1 - \\hat{\\alpha}_{k}\\Sigma_{kk} \\quad k \\in \\{1,\\ldots,K\\}.\n\\tag{11}\\]\nTipping (2001) likewise demonstrates that, under certain circumstances, \\(\\hat{\\alpha}_k \\approx \\infty\\), under which condition the prior distribution of \\(\\beta_k \\vert \\alpha_k\\) is approximately degenerate at 0 (as is the consequent posterior). We provide an algorithm for the estimation of an ARD model, given the above updating equations:\n\nInitialization. Enter a valid response vector \\(\\mathrm{y}\\) and a valid predictor matrix \\(\\boldsymbol{\\Phi}\\). Set a cutoff value \\(M &gt; 0\\) such that, if \\(\\hat{\\alpha}_k &gt; M\\), \\(\\hat{\\beta}_k = 0\\). Set initial values for the estimators \\(\\hat{\\tau} = \\tau_0\\) and \\(\\hat{\\alpha} = \\alpha_0\\). Thereafter, perform the following steps in sequence.\n\nCompute the quantities in Equation 10 and Equation 11.\nUpdate \\(\\hat{\\beta}_k\\) (for variables still in the model) according to Equation 7.\nUpdate \\(\\hat{\\alpha}_k\\) (for variables still in the model) and \\(\\hat{\\tau}\\) according to Equation 8 and Equation 9.\nFor every \\(k \\in \\{1,\\ldots,K\\}\\) if \\(\\hat{\\alpha}_{k} &gt; M\\), remove the \\(k\\)-th variable from the model.\nCheck the convergence of the estimates. If convergence has occurred, stop. Else, return to step (1).\n\n\nThe function ard_model() in the chunk below performs the estimating procedure.\n\n\nShow R code\nard_model = function(y,\n                     Phi,\n                     tau_ini = 1,                             \n                     alpha_ini = rep(x = 1,\n                                     times = ncol(Phi)),\n                     max_iter = 1e3,\n                     tol = 1e-8,\n                     cutoff = 1e4)\n{\n  time = Sys.time()\n  n = length(y)\n  K = ncol(Phi)\n  \n  tau = tau_ini\n  alpha = alpha_ini\n  keep = !logical(K)\n  \n  beta_est = rep(0, times = K)\n  gamma_est = rep(0, times = K)\n  \n  iter = 1\n  cond = Inf\n  beta_est0 = beta_est + Inf\n  \n  while((iter &lt; max_iter) & (abs(cond) &gt; tol))\n  {\n    Phi_keep = Phi[,keep]\n    \n    A = diag(alpha[keep])\n    \n    Sigma = solve(A + tau * t(Phi_keep) %*% Phi_keep)\n    \n    beta_est[keep] = tau * Sigma %*% t(Phi_keep) %*% y\n    \n    gamma_est[keep] = 1 - alpha[keep] * diag(Sigma)\n    \n    alpha[keep] = gamma_est[keep] / (beta_est[keep] ** 2)\n    tau = (n - sum(gamma_est[keep])) / sum((y - Phi_keep %*% beta_est[keep]) ** 2)\n    \n    keep = ifelse(test = alpha &lt; cutoff, yes = T, no = F)\n    \n    beta_est[!keep] = 0\n    alpha[!keep] = Inf\n    \n    iter %&lt;&gt;% + 1\n    \n    cond = sum((beta_est - beta_est0) ** 2) \n    beta_est0 = beta_est\n  }\n  \n  fitted_values = Phi[,keep] %*% beta_est[keep]\n  R2_adj0 = 1 - ((n - 1) / (n - length(gamma_est[keep]))) * (sum((y - fitted_values) ** 2) / sum((y - mean(y)) ** 2))\n  R2_adj = 1 - ((n - 1) / (n - sum(gamma_est[keep]))) * (sum((y - fitted_values) ** 2) / sum((y - mean(y)) ** 2))\n  \n  time %&lt;&gt;% {Sys.time() - .}\n  \n  return(list(iter = iter,\n              tau = tau,\n              alpha = alpha[keep],\n              beta = beta_est[keep],\n              gamma = gamma_est[keep],\n              keep = which(keep),\n              time = time,\n              fitted_values = fitted_values,\n              R2_adj = R2_adj,\n              R2_adj0 = R2_adj0))\n}\n\n\nBishop (2006) suggests utilizing \\(\\{\\gamma_k\\}\\), defined in Equation 11, as a measure of the model complexity, defining the effective degrees of freedom of the model as \\[\n  \\text{df}_\\text{eff} = \\sum^K_{k = 1}\\gamma_k.\n\\] Note that, if the \\(k\\)-th coefficient is included in the model, \\(\\gamma_k \\in [0,1]\\), whereas if its not included, \\(\\gamma_k = 0\\). In the following section we’ll utilize two distinct measures of quality of fit for our models. First, we define the usual adjusted \\(R\\) squared as \\[\n  R^2_\\text{adj} = 1 - \\frac{n - 1}{n - \\hat{K}_0} \\frac{\\sum^n_{i = 1}(y_i - \\hat{g}(t_i))^2}{\\sum^n_{i = 1}(y_i - \\bar{y}^2)},\n\\] where \\[\n  \\hat{K}_0 = K - \\sum^K_{k = 1} \\textbf{1}_{\\{0\\}}(\\hat{\\beta}_k),\n\\] that is, \\(\\hat{K}_0 \\in [0,K]\\) is the number of non-zero coefficients in the estimated model. We additionally define \\[\n  \\widetilde{R}^2_\\text{adj} = 1 - \\frac{n - 1}{n - \\text{df}_\\text{eff}} \\frac{\\sum^n_{i = 1}(y_i - \\hat{g}(t_i))^2}{\\sum^n_{i = 1}(y_i - \\bar{y}^2)}.\n\\] Note that \\(\\text{df}_\\text{eff} \\in [0,K_0]\\), hence \\(\\widetilde{R}^2_\\text{adj} \\geq R^2_\\text{adj}\\)."
  },
  {
    "objectID": "vignettes/ardRepresentation.html#fixed-point-updates",
    "href": "vignettes/ardRepresentation.html#fixed-point-updates",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Fixed Point Updates",
    "text": "Fixed Point Updates\nA"
  },
  {
    "objectID": "vignettes/ardRepresentation.html#expectation-maximization-approach",
    "href": "vignettes/ardRepresentation.html#expectation-maximization-approach",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Expectation Maximization Approach",
    "text": "Expectation Maximization Approach\nB"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Felipe Ferreira",
    "section": "",
    "text": "I’m a Statistician, having received my bachelor’s degree in Statistics (2022) from the Federal University of Juiz de Fora and my Master’s degree in Statistics (2024) from the University of São Paulo. My main research interests are Functional Data Analysis and Approximate Inference.\nCheck out my undergraduate thesis (in Portuguese) or my Master’s thesis."
  },
  {
    "objectID": "solutions.html",
    "href": "solutions.html",
    "title": "Exercise Solutions",
    "section": "",
    "text": "This page consists of exercise solutions of books I have been working through.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntrodução à Inferência Estatística\n\n\nHeleno Bolfarine and Mônica Carneiro Sandoval\n\n\n\n\n\n\n\n\nJul 2, 2025\n\n\nFelipe Toledo Ferreira\n\n\n\n\n\n\n\n\n\n\n\n\nPattern Recognition and Machine Learning\n\n\nChristopher M. Bishop\n\n\n\n\n\n\n\n\nJul 2, 2025\n\n\nFelipe Toledo Ferreira\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "vignettes/ardRepresentation.html#application-i",
    "href": "vignettes/ardRepresentation.html#application-i",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Application I",
    "text": "Application I\n\n\nShow R code\nind = 4\nmcycle_ard = ard_model(y = mcycle$accel,\n                       Phi = mcycle_fda[[ind]]$design_matrix,\n                       cutoff = 1e-2)\n(mcycle_ard %&gt;% {data.frame(eval_x = eval_x,\n                           eval_values = mcycle_fda[[ind]]$design_matrixf[,.$keep] %*% .$beta)} |&gt;\n  ggplot(mapping = aes(x = eval_x,\n                       y = eval_values)) +\n  geom_line(col = \"blue\") +\n  (mcycle |&gt; geom_point(inherit.aes = F,\n                        mapping = aes(x = times,\n                                      y = accel))) +\n  (mcycle_fda[[ind]] %&gt;% {data.frame(eval_x = eval_x,\n                                   eval_values = .$design_matrixf %*% .$coefficients)} |&gt;\n     geom_line(inherit.aes = F,\n               mapping = aes(x = eval_x,\n                             y = eval_values),\n               col = \"red\")) +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125)) +\n  labs(x = substitute(paste(\"Time after impact (in \", italic(\"ms\"), \")\")),\n       y = substitute(paste(\"Acceleration (in \", italic(\"g\"), \")\")))) /\n(coef_plots[[ind]]$g +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125)) +\n  (mcycle_ard %&gt;% {data.frame(x = .$keep,\n                              y = .$beta) |&gt;\n      geom_point(inherit.aes = F,\n                 mapping = aes(x = x,\n                               y = y),\n                 col = \"red\")})) + plot_layout(heights = c(2, 1))\nn = nrow(mcycle)\np = length(mcycle_ard$keep)\nR2_adj = 1 - ((n - 1) / (n - sum(mcycle_ard$gamma) - 1)) * sum((mcycle_ard$fitted_values - mcycle$accel) ** 2) / sum((mean(mcycle$accel) - mcycle$accel) ** 2)\n\n\n\n\n\n\n\n\nFigure 3: Left panel: sample estimated density of the fat percentage. Right panel: sample boxplot of the fat percentage.\n\n\n\n\n\nResulting in a \\(R^2_{\\text{adj}} \\approx 0.7847\\)"
  },
  {
    "objectID": "vignettes/ardRepresentation.html#application-ii",
    "href": "vignettes/ardRepresentation.html#application-ii",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Application II",
    "text": "Application II\nFor the Canadian weather application, as previously stated, we utilized \\(K = 51\\) Fourier basis functions, starting at \\(1/2\\) and ending at \\(365 + 1/2\\), with periodicity of \\(365\\). We adopted as the cutoff parameter the value of \\(\\log_{10} M = 4\\), though we also found that lower values (e.g. \\(\\log_{10} M = 2\\)) exhibit similar overall performance. Figure 7 illustrates the estimated mean functions.\n\n\nShow R code\ncanadianWeather_ard = lapply(X = seq_along(CanadianWeather$place),\n                             FUN = function(x){\n                               ard_model(y = CanadianWeather$dailyAv[ , x, 1],\n                                         Phi = canadianWeather_fda[[x]]$design_matrix,\n                                         cutoff = 1e4)\n                             })\n(canadianWeather_ard %&gt;% {lapply(X = seq_along(CanadianWeather$place),\n        FUN = function(x){\n          data.frame(eval_x = eval_x_canadian,\n                     values = canadianWeather_fda[[x]]$design_matrixf[,.[[x]]$keep] %*% .[[x]]$beta,\n                     x = factor(CanadianWeather$place[x],\n                               levels = unique(CanadianWeather$place)),\n                     region = factor(CanadianWeather$region[x],\n                               levels = unique(CanadianWeather$region)),\n                     row.names = NULL)\n        })} %&gt;% {do.call(what = rbind,\n                       args = .)} %&gt;%\n  ggplot(data = .,\n         mapping = aes(x = eval_x,\n                       y = values,\n                       group = x,\n                       col = region)) +\n    geom_line() +\n    theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125),\n        legend.position = \"bottom\") +\n  labs(x = \"Day of the year\",\n       y = substitute(paste(\"Temperature (in \", italic(\"°C\"), \")\")),\n       col = \"Region\") +\n  scale_color_discrete(type = viridis::viridis(n = 4)) +\n    scale_x_continuous(breaks = c(1, 100, 200, 300, 365)) +\n    geom_hline(yintercept = 0,\n                linetype = \"dashed\",\n                col = \"red\")) +\n  (lapply(X = seq_along(CanadianWeather$place),\n       FUN = function(x){\n         data.frame(values = CanadianWeather$dailyAv[ , x , 1],\n                    eval_x = 1:365,\n                    x = factor(CanadianWeather$place[x],\n                               levels = unique(CanadianWeather$place)),\n                     region = factor(CanadianWeather$region[x],\n                               levels = unique(CanadianWeather$region)),\n                     row.names = NULL)\n       }) %&gt;% {do.call(what = rbind,\n                       args = .)} %&gt;%\n  geom_line(inherit.aes = F,\n            data = .,\n            mapping = aes(x = eval_x,\n                          y = values,\n                          group = x,\n                          col = region)))\ntime_canadian_ARD = canadianWeather_ard %&gt;% {lapply(X = seq_along(CanadianWeather$place), FUN = function(x){.[[x]]$time})} |&gt; unlist() |&gt; sum()\np_basis_canadian_ARD = 100 * (canadianWeather_ard %&gt;% {lapply(X = seq_along(CanadianWeather$place), FUN = function(x){length(.[[x]]$keep)})} |&gt; unlist() |&gt; mean()) / K_canadian\nR2_adj_basis_canadian_ARD = canadianWeather_ard %&gt;% {lapply(X = seq_along(CanadianWeather$place), FUN = function(x){.[[x]]$R2_adj})} |&gt; unlist() |&gt; mean()\nR2_adj0_basis_canadian_ARD = canadianWeather_ard %&gt;% {lapply(X = seq_along(CanadianWeather$place), FUN = function(x){.[[x]]$R2_adj0})} |&gt; unlist() |&gt; mean()\n\n\n\n\n\n\n\n\nFigure 7: Estimated mean functions for each city in the Canadian weather data set, with \\(K = 51\\), obtained via the ARD procedure. The original observed data are likewise plotted.\n\n\n\n\n\nWe note that the ARD estimation procedure for all functions took approximately 4.8932 seconds to finish, and kept on average approximately \\(74.8459\\%\\) of the original basis functions per estimated mean function, resulting in an average adjusted \\(R\\) squared of \\(R^2_\\text{adj} \\approx 0.9968\\) and \\(\\widetilde{R}^2_\\text{adj} \\approx 0.9969\\). Figure 8 exhibits the coefficients corresponding to the estimated mean functions.\n\n\nShow R code\np_canadian_1 = (lapply(X = seq_along(CanadianWeather$place),\n       FUN = function(x){\n         canadianWeather_ard[[x]] %&gt;%\n         {data.frame(kept = .$keep,\n                    region = factor(CanadianWeather$region[x],\n                                    levels = unique(CanadianWeather$region)),\n                    beta = .$beta,\n                    row.names = NULL)}\n       }) %&gt;% do.call(what = rbind,\n                      args = .) |&gt;\n  ggplot(mapping = aes(x = kept,\n                       y = beta,\n                       col = region)) +\n  geom_point() +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125),\n        legend.position = \"bottom\") +\n  labs(x = substitute(paste(\"Basis index (\", italic(k), \")\")),\n       y = substitute(paste(\"Estimated coefficient (\", hat(beta)[italic(ik)], \")\")),\n       col = \"Region\") +\n  scale_color_discrete(type = viridis::viridis(4)) +\n    scale_x_continuous(breaks = c(1, 5)) +\n    geom_hline(col = \"red\", linetype = \"dashed\", yintercept = 0))\np_canadian_2 = (lapply(X = seq_along(CanadianWeather$place),\n       FUN = function(x){\n         canadianWeather_ard[[x]] %&gt;%\n         {data.frame(kept = .$keep,\n                    region = factor(CanadianWeather$region[x],\n                                    levels = unique(CanadianWeather$region)),\n                    beta = .$beta,\n                    row.names = NULL)}\n       }) %&gt;% do.call(what = rbind,\n                      args = .) |&gt;\n     subset(kept &gt; 5) |&gt;\n  ggplot(mapping = aes(x = kept,\n                       y = beta,\n                       col = region)) +\n  geom_point() +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125),\n        legend.position = \"none\",\n        axis.title.y = element_blank(),\n        axis.title.x = element_blank()) +\n  labs(x = substitute(paste(\"Basis index (\", italic(k), \")\")),\n       y = substitute(paste(\"Estimated coefficient (\", hat(beta)[italic(ik)], \")\")),\n       col = \"Region\") +\n  scale_color_discrete(type = viridis::viridis(4)) +\n    scale_x_continuous(breaks = c(6, 10, 20, 30, 40, 51)) +\n    geom_hline(col = \"red\", linetype = \"dashed\", yintercept = 0))\np_canadian_1 + inset_element(p_canadian_2, .132, .05, 1, 1) +\n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\n\n\nFigure 8: Panel I: Estimated coefficients for basis expansions of the Canadian weather data set, with \\(K = 51\\), obtained via the ARD procedure. Panel II: Inset zoom-in of the former Panel for basis indexes between 6 and 51.\n\n\n\n\n\nAs is expected when utilizing a Fourier basis expansion, the coefficients associated with basis’ with higher frequencies are smaller in absolute value, some close to zero. Nevertheless, we found that these basis’ are only removed from the model when the cutoff is lower than \\(\\log_{10} M = 0\\). Moreover, the model obtained with \\(\\log_{10} M = 0\\) has worsened average adjusted \\(R\\) squared. Consequently, we opted to stick with the cutoff of \\(\\log_{10} M = 4\\). Table 2 presents some diagnostic values obtained from the model (\\(R^2_\\text{adj}\\) is denoted as R2 I and \\(\\widetilde{R}^2_\\text{adj}\\) is denoted as R2 II), discriminated by city. Once more, the results followed closely those obtained in Cruz, Souza, and Sousa (2024).\n\n\n\n\n\n\n\n\n\n\nTable 2: Some diagnostic values obtained by the application of the ARD procedure, discriminated by city."
  },
  {
    "objectID": "vignettes/ardRepresentation.html#simulation-study-i",
    "href": "vignettes/ardRepresentation.html#simulation-study-i",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Simulation Study I",
    "text": "Simulation Study I\nFor each simulation study scenario, we replicated \\(\\textbf{R} = 1000\\) data sets, obtaining a model for each via Automatic Relevance Determination. Let \\(K\\) denote the number of basis functions, \\(K_0\\) denote the number of non-zero coefficients, \\(\\hat{\\beta}_{ijr}(N,n)\\) denote the estimated coefficient corresponding to the \\(j\\)-th basis function of the \\(i\\)-th functional observation obtained on the \\(r\\)-th replication of the scenario with number of functional observations \\(N\\) and number of sampled instants \\(n\\) (analogously for \\(\\hat{\\tau}_{ir}(N,n)\\)) we consider of interest the following measures: \\[\n  \\hat{\\beta}_{jr}(N,n) = \\frac{1}{N}\\sum^N_{i = 1}\\hat{\\beta}_{ijr}(N,n)\n\\tag{12}\\] \\[\n  \\hat{\\tau}_r(N,n) = \\frac{1}{N}\\sum^N_{i = 1}\\hat{\\tau}_{ir}(N,n)\n\\tag{13}\\] \\[\n  \\widehat{\\text{SEN}}_r(N,n) = \\frac{1}{N (K - K_0)}\\Bigg(\\sum_{\\{(i,j) : \\beta_{ij}(n) \\neq 0\\}}\\textbf{1}_{\\{0\\}^C}(\\hat{\\beta}_{ijr}(N,n))\\Bigg)\n\\tag{14}\\]\n\\[\n  \\widehat{\\text{SPE}}_r(N,n) = \\frac{1}{NK_0}\\sum_{\\{(i,j) : \\beta_{ij} = 0\\}}\\textbf{1}_{\\{0\\}}(\\hat{\\beta}_{ijr}(N,n))\n\\tag{15}\\] \\[\n  \\widehat{\\text{ACC}}_r(N,n) = \\frac{1}{NK}\\Bigg(\\sum_{\\{(i,j) : \\beta_{ij} = 0\\}}\\textbf{1}_{\\{0\\}}(\\hat{\\beta}_{ijr}(N,n)) + \\sum_{\\{(i,j) : \\beta_{ij} \\neq 0\\}}\\textbf{1}_{\\{0\\}^c}(\\hat{\\beta}_{ijr}(N,n))\\Bigg).\n\\tag{16}\\]\nEquation 12 and Equation 13 measure the average values obtained for the \\(j\\)-th coefficient in the basis expansion and the average precision parameter, respectively, for the \\(r\\)-th replication. Equation 14 measures the sensibility (percentage of non-zero coefficients which were estimated as non-zero) of the \\(r\\)-th replication. Equation 15 measures the specificity (percentage of zero coefficients which were estimated as zero) of the \\(r\\)-th -replication. Lastly, Equation 16 measures the overall accuracy of the \\(r\\)-th -replication. The R chunk provided below performs \\(\\textbf{R} = 1000\\) replications of the nine scenarios for the first Simulation framework. For this simulation we adopted as the cutoff in the ARD procedure the value of \\(\\log_{10} M = 1\\).\n\n\nShow R code\nset.seed(23121998)\nN_sim_1 = rep(c(10, 100, 1000), each = 3)\nn_sim_1 = rep(c(10, 100, 1000), times = 3)\ndf_sim_1 = \n  lapply(X = seq_along(N_sim_1),\n         function(q){\n          N_1 = N_sim_1[q]\n          n_1 = n_sim_1[q]\n          t_1 = seq(from = 0, to = 1, length.out = n_1)\n          Phi_1 = create.bspline.basis(rangeval = c(0, 1),\n                                       norder = 4,\n                                       nbasis = K_1) %&gt;%\n            getbasismatrix(evalarg = t_1,\n                           basisobj = .,\n                           nderiv = 0)\n          mean_1 = Phi_1 %*% beta_1\n          sim_data_total_1 = lapply(X = 1:replication_1, FUN = function(z){\n          sim_data_1 = lapply(X = 1:N_1,\n                            FUN = function(x){\n                              beta_est = numeric(length = K_1)\n                              y = mean_1 + (tau_1 ** (-1/2)) * rnorm(n = n_1)\n                              est = ard_model(y = y,\n                                              Phi = Phi_1,\n                                              cutoff = 1e1)\n                              beta_est[est$keep] = est$beta\n                              sen = sum(est$keep %in% beta_in_1)\n                              spe = sum((1:K_1)[-c(est$keep)] %in% beta_out_1)\n                              acc = sen + spe\n                              tau_est = est$tau\n                              return(c(beta_est / N_1, tau_est / N_1, sen, spe, acc))\n                            }) %&gt;% do.call(what = rbind,\n                                           args = .) |&gt; apply(MARGIN = 2, FUN = sum)\n        }) %&gt;% do.call(what = rbind,\n                       args = .) |&gt; as.data.frame()\n        colnames(sim_data_total_1) = c(paste(rep(\"beta\", K_1),\n                                             1:K_1, sep = \"\"),\n                               \"tau\",\n                               \"sen\",\n                               \"spe\",\n                               \"acc\")\n        sim_data_total_1 %&lt;&gt;% mutate(sen = sen / (N_1 * length(beta_in_1)),\n                             spe = spe / (N_1 * length(beta_out_1)),\n                             acc = acc / (N_1 * K_1))\n        sim_data_total_1 %&lt;&gt;% melt()\n        sim_data_total_1 %&lt;&gt;% mutate(n = n_1, N = N_1)\n    return(sim_data_total_1)\n}) %&gt;% do.call(what = rbind,\n               args = .)\n\n\n\n\nShow R code\ndf_sim_1 %&lt;&gt;% mutate(n = factor(n), N = factor(N))\ndf_sim_1 |&gt; subset(grepl(\"beta\", variable)) |&gt;\n  ggplot(mapping = aes(x = n,\n                       fill = N,\n                       y = value)) +\n  geom_boxplot() +\n  facet_wrap(vars(variable)) +\n  theme_classic() +\n  scale_fill_discrete(type = viridis::viridis(3)) +\n  labs(x = substitute(paste(\"Number of sampled instants (\", italic(n), \")\")),\n       fill = substitute(paste(\"Number of sampled functions (\", italic(N), \")\")),\n       y = \"Estimated\") +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nFigure 9: \\(N = 100\\) functions sampled at \\(n = 100\\) instants according to Equation 5\n\n\n\n\n\n\n\nShow R code\ndf_sim_1 |&gt; group_by(variable, N, n) |&gt; summarise(mean = round(mean(value), 4),\n                                                  median = round(median(value), 4),\n                                                  sd = round(sd(value), 4)) |&gt;\n  DT::datatable(colnames = c(\"Variable\", \"N\", \"n\", \"Mean\", \"Median\", \"Standard Deviation\"))\n#&gt; `summarise()` has grouped output by 'variable', 'N'. You can override using the\n#&gt; `.groups` argument."
  },
  {
    "objectID": "vignettes/ardRepresentation.html#simulation-study-ii",
    "href": "vignettes/ardRepresentation.html#simulation-study-ii",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Simulation Study II",
    "text": "Simulation Study II\nThe R chunk provided below performs \\(\\textbf{R} = 1000\\) of the nine scenarios for the second Simulation framework. For this simulation we adopted as the cutoff in the ARD procedure the value of \\(\\log_{10} M = 1\\).\n\n\nShow R code\nset.seed(12231998)\nN_sim_2 = rep(c(10, 100, 1000), each = 3)\nn_sim_2 = rep(c(10, 100, 1000), times = 3)\ndf_sim_2 = \n  lapply(X = seq_along(N_sim_2),\n         function(q){\n          N_2 = N_sim_2[q]\n          n_2 = n_sim_2[q]\n          t_2 = seq(from = 0, to = 2 * pi, length.out = n_2)\n          Phi_2 = create.fourier.basis(rangeval = c(0, 2 * pi),\n                                       period = 2 * pi,\n                                       nbasis = K_2) %&gt;%\n            getbasismatrix(evalarg = t_2,\n                           basisobj = .,\n                           nderiv = 0)\n          mean_2 = cos(t_2) + sin(2 * t_2)\n          sim_data_total_2 = lapply(X = 1:replication_2, FUN = function(z){\n          sim_data_2 = lapply(X = 1:N_2,\n                            FUN = function(x){\n                              beta_est = numeric(length = K_2)\n                              y = mean_2 + (tau_2 ** (-1/2)) * rnorm(n = n_2)\n                              est = ard_model(y = y,\n                                              Phi = Phi_2,\n                                              cutoff = 1e1)\n                              beta_est[est$keep] = est$beta\n                              sen = sum(est$keep %in% beta_in_2)\n                              spe = sum((1:K_2)[-c(est$keep)] %in% beta_out_2)\n                              acc = sen + spe\n                              tau_est = est$tau\n                              return(c(beta_est / N_2, tau_est / N_2, sen, spe, acc))\n                            }) %&gt;% do.call(what = rbind,\n                                           args = .) |&gt; apply(MARGIN = 2, FUN = sum)\n        }) %&gt;% do.call(what = rbind,\n                       args = .) |&gt; as.data.frame()\n        colnames(sim_data_total_2) = c(paste(rep(\"beta\", K_2),\n                                             1:K_2, sep = \"\"),\n                               \"tau\",\n                               \"sen\",\n                               \"spe\",\n                               \"acc\")\n        sim_data_total_2 %&lt;&gt;% mutate(sen = sen / (N_2 * length(beta_in_2)),\n                             spe = spe / (N_2 * length(beta_out_2)),\n                             acc = acc / (N_2 * K_2))\n        sim_data_total_2 %&lt;&gt;% melt()\n        sim_data_total_2 %&lt;&gt;% mutate(n = n_2, N = N_2)\n    return(sim_data_total_2)\n}) %&gt;% do.call(what = rbind,\n               args = .)\n\n\n\n\nShow R code\ndf_sim_2 %&lt;&gt;% mutate(n = factor(n), N = factor(N))\ndf_sim_2 |&gt; subset(grepl(\"beta\", variable)) |&gt;\n  ggplot(mapping = aes(x = n,\n                       fill = N,\n                       y = value)) +\n  geom_boxplot() +\n  facet_wrap(vars(variable)) +\n  theme_classic() +\n  scale_fill_discrete(type = viridis::viridis(3)) +\n  labs(x = substitute(paste(\"Number of sampled instants (\", italic(n), \")\")),\n       fill = substitute(paste(\"Number of sampled functions (\", italic(N), \")\")),\n       y = \"Estimated\") +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nFigure 10: \\(N = 100\\) functions sampled at \\(n = 100\\) instants according to Equation 5\n\n\n\n\n\n\n\nShow R code\ndf_sim_2 |&gt; group_by(variable, N, n) |&gt; summarise(mean = round(mean(value), 4),\n                                                  sd = round(sd(value), 4),\n                                                  median = round(median(value), 4)) |&gt;\n  DT::datatable(colnames = c(\"Variable\", \"N\", \"n\", \"Mean\", \"Median\", \"Standard Deviation\"))\n#&gt; `summarise()` has grouped output by 'variable', 'N'. You can override using the\n#&gt; `.groups` argument."
  },
  {
    "objectID": "vignettes/ardRepresentation.html#simulation-study-iii",
    "href": "vignettes/ardRepresentation.html#simulation-study-iii",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Simulation Study III",
    "text": "Simulation Study III"
  },
  {
    "objectID": "vignettes/ardRepresentation.html#footnotes",
    "href": "vignettes/ardRepresentation.html#footnotes",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe avoid denoting the accelerometer measurements as \\(y(t_j)\\) since, as stated, there are measurement times \\(t_j\\) which appear repeatedly, with distinct \\(y_j\\), such that writing \\(y(t_j) = y_j\\) would result in inconsistencies. The same is true for the random variables from which these observations are drawn, which we denote as \\(Y_j\\).↩︎\nThe equivalency between Equation 1 and Equation 2 is a consequence of the assumption of normality for the random errors in Equation 2 (although this equivalency would hold so long as \\(\\varepsilon_j\\) had mean zero). The form in Equation 2 is more appealing due to its inherent distinction between the deterministic and stochastic components, however, the formulation in Equation 1 (coupled with distributional assumptions on \\(Y_j\\)) is adopted under the generic framework of ‘generalized linear models’ (or particularly, for function estimation, the framework of ‘generalized additive models’; see Hastie and Tibshirani (1986)).↩︎\nThis behavior is expected: as the object is approximately inert prior to the impact, likewise, in the immediate milliseconds following impact, it is approximately inert, eventually gaining acceleration, which dissipates as time passes, and the object eventually returns to inertia.↩︎\nUnlike the Motorcycle data set, for any fixed city \\(i\\) in the Canadian weather data set, the observation instants do not repeat and are all equally spaced. Also importantly, they are the same across all cities (that is, \\(t_{ij} = t_j, \\forall i \\in \\{1,\\ldots,N\\}\\)). This likewise implies that the matrix \\(\\boldsymbol{\\Phi}_i\\) defined later is the same independent of the subscript \\(i\\). We opt to not remove the subscript in this data set to preserve the generality of the content. When the observation instants satisfy these conditions, the sampling design is said to be regular.↩︎\nHowever, it’s expected that some mean temperature functions present roughly similar behavior due to geographical proximity and climate similarity of the corresponding cities. These similarities have been studied in applications in functional data clustering, as seen in Xian et al. (2024).↩︎"
  },
  {
    "objectID": "vignettes/ardRepresentation.html#sec-app-mcycle",
    "href": "vignettes/ardRepresentation.html#sec-app-mcycle",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Application I",
    "text": "Application I\nFor the Motorcycle data set application, we utilize the cubic B-spline expansion composed of \\(K = 20\\) basis functions. We first set the cutoff \\(M\\) in the ARD algorithm to an arbitrarily large value (\\(\\log_{10}M = 4\\)), in which case the the estimated model had \\(\\hat{K}_0 = 7\\) non-zero coefficients. However, by setting the cutoff to a lower value (\\(\\log_{10} M = -2\\)) we found the model’s adjusted \\(R\\) squared had improved, with the model having only \\(\\hat{K}_0 = 5\\) non-zero coefficients. It may therefore be of interest to view the cutoff value \\(M\\) as a tuning parameter. Figure 6 illustrates the estimated model.\n\n\nShow R code\nind = 4\nmcycle_ard = ard_model(y = mcycle$accel,\n                       Phi = mcycle_fda[[ind]]$design_matrix,\n                       cutoff = 1e-2)\n(mcycle_ard %&gt;% {data.frame(eval_x = eval_x,\n                           eval_values = mcycle_fda[[ind]]$design_matrixf[,.$keep] %*% .$beta)} |&gt;\n  ggplot(mapping = aes(x = eval_x,\n                       y = eval_values,\n                       col = \"ARD estimate\")) +\n  geom_line() +\n  (mcycle |&gt; geom_point(inherit.aes = F,\n                        mapping = aes(x = times,\n                                      y = accel))) +\n  (mcycle_fda[[ind]] %&gt;% {data.frame(eval_x = eval_x,\n                                   eval_values = .$design_matrixf %*% .$coefficients)} |&gt;\n     geom_line(inherit.aes = F,\n               mapping = aes(x = eval_x,\n                             y = eval_values,\n                             col = \"ML estimate\"))) +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125),\n        legend.position = \"bottom\") +\n  labs(x = substitute(paste(\"Time after impact (in \", italic(\"ms\"), \")\")),\n       y = substitute(paste(\"Acceleration (in \", italic(\"g\"), \")\")),\n       col = \"\") +\n    geom_hline(col = \"red\",\n               linetype = \"dashed\",\n               yintercept = 0) +\n    scale_color_discrete(type = viridis::viridis(2))) /\n  (guide_area()) / \n(coef_plots[[ind]]$g +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125)) +\n  (mcycle_ard %&gt;% {data.frame(x = .$keep,\n                              y = .$beta) |&gt;\n      geom_point(inherit.aes = F,\n                 mapping = aes(x = x,\n                               y = y),\n                 col = \"red\")})) + plot_layout(heights = c(4, .5, 2)) +\n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\n\n\nFigure 6: Panel I: Estimated mean functions for the Motorcycle data set, with \\(K = 20\\), obtained via the ARD procedure and Maximum Likelihood (ML). Panel II: Estimated coefficients for basis expansions of the Motorcycle data set, with \\(K = 20\\). The black points are those obtained via ML, whilst the red points are those obtained via ARD. Estimates equal to zero are ommited.\n\n\n\n\n\nThe observed adjusted \\(R\\) squared values were \\(R^2_\\text{adj} \\approx 0.7858\\) and \\(\\widetilde{R}^2_\\text{adj} \\approx 0.7864\\). Note that the ARD procedure selects basis functions which model the interval where the acceleration showcases a valley followed by a peak, effectively capturing the most apparent trends in the data whilst utilizing only 25% of the originally proposed basis functions. The basis functions which correspond to the beginning of the mean function (around 15\\(ms\\) after impact), where the data is approximately inert, are removed, as are those which model the tail end of the mean function (around 40\\(ms\\) after impact), as was expected. These results follow closely those obtained in Cruz, Souza, and Sousa (2024)."
  }
]