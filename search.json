[
  {
    "objectID": "vignettes.html",
    "href": "vignettes.html",
    "title": "R Vignettes",
    "section": "",
    "text": "This page consists of some Vignettes I have developed utilizing the R programming language.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Relevance Determination for Functional Data Representation\n\n\n…a Vignette on Functional Data Analysis\n\n\nAn approach to functional data representation based on automatic relevance determination.\n\n\n\n\n\nJul 7, 2025\n\n\nFelipe Toledo Ferreira\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "vignettes/ardRepresentation.html",
    "href": "vignettes/ardRepresentation.html",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "",
    "text": "Last updated 2025/07/07."
  },
  {
    "objectID": "vignettes/ardRepresentation.html#data",
    "href": "vignettes/ardRepresentation.html#data",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Data",
    "text": "Data\nFor this particular study, we consider two distinct data sets, the Motorcycle data set and the Canadian weather data set: the former is frequently studied in the overall spline smoothing context (as well as overall studies in Nonparametric Statistics), whilst the latter is most often used in the specific context of Functional Data Analysis. We will also present a brief simulation study. The R chunk provided below loads the relevant packages utilized throughout.\n\n\nShow R code\nlibrary(\"tidyverse\")  ## R package for tidy data manipulation and visualization\nlibrary(\"extrafont\")  ## R package for using nice fonts in figures\nlibrary(\"fda\")        ## R package for functional data analysis\nlibrary(\"MASS\")\nlibrary(\"magrittr\")   ## R package for pipes\nlibrary(\"patchwork\")  ## R package for plot composition\nlibrary(\"reshape2\")\nlibrary(\"rstan\")\n\n\n\nThe Motorcycle data set\nThe Motorcycle data set (seen in Silverman (1985), and available in the R programming language in the package MASS) consists of \\(n = 133\\) measurements obtained through an experiment which simulated motorcycle accidents with the goal of determining the efficacy of crash helmets: an accelerometer was fitted to an object which was later submitted to repeated mechanical impacts, and the resulting acceleration was measured in the instants subsequent to the impacts. We denote the measurements of the time (in milliseconds) elapsed since the impact occurs as \\(t_j\\), and the acceleration (in g) measured at the corresponding instant as \\(y_j\\), hence we denote the full data set as \\(\\{t_j, y_j\\}^n_{j = 1}\\). Some observations can be made with respect to the set of instants \\(\\{t_j\\}^n_{j = 1}\\): as the data set was gathered through repeat experiments, the measured instants are not equally spaced, and some appear multiple times (with distinct corresponding \\(y_j\\)). Let \\(Y_j\\) denote a random variable corresponding to the \\(j\\)-th accelerometer measurement, we assume that\n\\[\n  \\mathbb{E}[Y_j] = g(t_j) \\quad j \\in \\{1,\\ldots,n\\}.\n\\tag{1}\\]\nThat is, we assume that albeit the accelerometer measurements are randomly distributed, they present some regularity in the form of a mean, which is dependent on the instant at which the measurement is made, expressed as the mean function \\(g(\\cdot)\\)1. In particular, we’ll assume that \\(Y_j \\sim \\text{Normal}(g(t_j), 1/\\tau)\\), \\(\\tau &gt; 0\\), or equivalently\n\\[\n  y_j = g(t_j) + \\tau^{-1/2}\\varepsilon_j \\quad j \\in \\{1,\\ldots,n\\},\n\\tag{2}\\]\nwhere \\(\\varepsilon_j \\sim \\text{Normal}(0,1), j \\in \\{1,\\ldots,n\\}\\)2. The first term on the r.h.s. of Equation 2 is the ‘deterministic component’, whilst the second term on the r.h.s. of Equation 2 is the ‘stochastic component’. Our goal is to construct an estimator \\(\\hat{g}(\\cdot)\\) for the deterministic component. Naturally, searching for an optimal \\(\\hat{g}(\\cdot) \\in \\mathcal{G}\\), where \\(\\mathcal{G}\\) is an arbitrary functional space (e.g. the space of square integrable functions) is computationally unfeasible, as \\(\\mathcal{G}\\) would likely be infinite-dimensional. For that purpose, first we will assume that \\(g(\\cdot)\\) may be written as the linear combination of a set of basis functions \\(\\{\\phi_k(\\cdot)\\}_{k = 1}^{K}\\) of the following form\n\\[\n  g(t) = \\sum^K_{k = 1}\\beta_{k} \\phi_k(t),\n\\tag{3}\\]\nfor some \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^{K}\\). This allows us to recontextualize the usually infinite-dimensional problem of estimating \\({g}(\\cdot) \\in \\mathcal{G}\\) as a finite-dimensional problem of estimating \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^K\\). Ideally, the set of basis functions \\(\\{\\phi_k(\\cdot)\\}_{k = 1}^K\\) would be chosen so as to both reflect properties we believe the function \\(g(\\cdot)\\) possesses (e.g., continuity to the \\(M\\)-th order) and to provide a sufficiently flexible foundation which may approximate several distinct forms of \\(g(\\cdot)\\) (there are also data-driven choices for basis functions, such as via functional principal components analysis). A natural choice for a set of basis functions are those generated in the B-spline framework (see De Boor (1978)). In particular, we’ll consider cubic B-splines with equidistant knots (with the first knot defined as \\(\\min_j\\{t_j\\}\\) and the last knot as \\(\\max_j\\{t_j\\}\\)), such that the number of basis functions in a basis set is \\(K\\) (including boundary B-splines), with \\(K \\in \\{5,10,15,20,25,30\\}\\). Let \\(\\boldsymbol{\\Phi} \\in \\mathbb{R}^{K \\times K}\\) and \\(\\boldsymbol{\\phi}(t) \\in \\mathbb{R}^K\\) be defined as \\[\n  \\boldsymbol{\\Phi} = \\begin{pmatrix}\n    \\phi_{1}(t_1) & \\phi_{2}(t_1) & \\dots & \\phi_{K}(t_1) \\\\\n    \\phi_{1}(t_2) & \\phi_{2}(t_2) & \\dots & \\phi_{K}(t_2) \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\phi_{1}(t_n) & \\phi_{2}(t_n) & \\dots & \\phi_{K}(t_n)\n  \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{\\phi}(t) = \\begin{pmatrix}\n    \\phi_1(t) \\\\\n    \\phi_2(t) \\\\\n    \\vdots \\\\\n    \\phi_K(t)\n  \\end{pmatrix},\n\\tag{4}\\] we determine the estimates for the coefficients \\(\\boldsymbol{\\beta}\\) via maximum likelihood estimation as \\[\n  \\begin{aligned}\n    \\hat{\\boldsymbol{\\beta}}_\\text{ML} & = \\arg \\max_{\\boldsymbol{\\beta}\\in\\mathbb{R}^K} \\Bigg\\{-\\frac{n}{2}\\ln(2 \\pi) + \\frac{n}{2} \\ln(\\tau) -\\frac{\\tau}{2}\\sum^n_{j = 1}(y_j - \\boldsymbol{\\beta}^\\top \\boldsymbol{\\phi}(t_j))^2 \\Bigg\\} \\\\\n    & = \\arg \\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^K} \\Bigg\\{\\sum^n_{j = 1}(y_j - \\boldsymbol{\\beta}^\\top \\boldsymbol{\\phi}(t_j))^2\\Bigg\\} \\\\\n    \\hat{\\boldsymbol{\\beta}}_\\text{ML} & = (\\boldsymbol{\\Phi}^\\text{T} \\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^\\text{T}\\mathrm{y},\n  \\end{aligned}\n\\] assuming \\(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\) is positive-definite. Therefore, the estimated mean function \\(\\hat{g}(\\cdot)\\) evaluated at a point \\(s\\) is defined as \\[\n  \\hat{g}(s) = \\boldsymbol{\\hat{\\beta}}^\\text{T}_{\\text{ML}} \\boldsymbol{\\phi}(s).\n\\] Figure 1 illustrates the estimated mean functions \\(\\hat{g}(\\cdot)\\) obtained via maximum likelihood for each \\(K\\).\n\n\nShow R code\nset.seed(747)\nK = c(5, 10, 15, 20, 25, 30)\neval_x = mcycle %&gt;% {seq(from = min(.$times), to = max(.$times), length.out = 1e3 + 1)}\nmcycle_fda = \n  lapply(X = seq_along(K),\n         FUN = function(x){\n           design_basis = create.bspline.basis(rangeval = range(mcycle$times),\n                                               norder = 4,\n                                               nbasis = K[x])\n           design_matrix = design_basis %&gt;%\n             getbasismatrix(evalarg = mcycle$times,\n                            basisobj = .,\n                            nderiv = 0)\n           design_matrixf = design_basis %&gt;%\n             getbasismatrix(evalarg = eval_x,\n                            basisobj = .,\n                            nderiv = 0)\n           coefficients = design_matrix %&gt;%\n             {solve(a = t(.) %*% .,\n                    b = t(.) %*% mcycle$accel)}\n           colnames(design_matrix) = 1:K[x]\n           colnames(design_matrixf) = 1:K[x]\n           return(list(K = K[x],\n                       design_matrix = design_matrix,\n                       design_matrixf = design_matrixf,\n                       coefficients = coefficients,\n                       fitted_values = design_matrix %*% coefficients,\n                       eval_values = design_matrixf %*% coefficients))\n           })\n(lapply(X = seq_along(K),\n       FUN = function(x){\n         data.frame(values = mcycle_fda[[x]]$eval_values,\n                    eval_x = eval_x,\n                    x = factor(K[x],\n                               levels = K))\n       }) %&gt;% {do.call(what = rbind,\n                       args = .)} %&gt;%\n  ggplot(data = .,\n         mapping = aes(x = eval_x,\n                       y = values,\n                       group = x,\n                       col = x)) +\n  geom_line() +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125),\n        legend.position = \"bottom\") +\n  labs(x = substitute(paste(\"Time after impact (in \", italic(\"ms\"), \")\")),\n       y = substitute(paste(\"Acceleration (in \", italic(\"g\"), \")\")),\n       col = substitute(italic(K))) +\n  scale_color_discrete(type = viridis::viridis(n = 6)) +\n    geom_hline(col = \"red\",\n               linetype = \"dashed\",\n               yintercept = 0) +\n  (mcycle |&gt; geom_point(inherit.aes = F,\n                        mapping = aes(x = times,\n                                      y = accel)))) / guide_area() + plot_layout(heights = c(4, .25))\n\n\n\n\n\n\n\n\nFigure 1: Estimated mean functions for the Motorcycle data set, for \\(K \\in \\{5,10,15,20,25,30\\}\\). The black dots represent the sampled data, and the dashed red line is represents to \\(y = 0\\).\n\n\n\n\n\nSome remarks can be made with respect to the results of Figure 1: first, we note that for small \\(K\\) (say \\(K \\in \\{5,10\\}\\)), the estimated mean function \\(\\hat{g}(\\cdot)\\) underfits the data, and is unable to capture localized trends. By contrast, for large \\(K\\) (say \\(K \\in \\{25,30\\}\\)), the estimated mean function overfits the data, extrapolating trends which are potentially spurious. This behavior is expected, and is well known in Statistics as the ‘bias-variance tradeoff’. A more interesting remark can be made after we observe Figure 2, which illustrates the estimated coefficient values \\(\\{\\hat{\\boldsymbol{\\beta}}_\\text{ML}\\}_k\\) for each \\(K\\).\n\n\nShow R code\ncoef_plots =\n  lapply(X = seq_along(K),\n         FUN = function(x){\n           g = mcycle_fda[[x]] %&gt;%\n             {data.frame(x = seq_along(.$coefficients),\n                         y = .$coefficients) |&gt;\n                 ggplot(mapping = aes(x = x,\n                                      y = y)) +\n                 geom_line(col = \"black\",\n                           linetype = \"dashed\") +\n                 geom_point() +\n                 geom_hline(yintercept = 0,\n                            linetype = \"dashed\",\n                            col = \"red\") +\n                 scale_x_continuous(breaks = c(1, seq(from = 5, to = K[x], by = 5))) +\n                 labs(x = substitute(paste(\"Basis index (\", italic(k), \")\")),\n                      y = substitute(paste(\"Estimated coefficient (\", hat(beta)[k], \")\")))}\n           return(list(g = g))\n         })\ncoef_plots[[1]]$g /\ncoef_plots[[2]]$g /\ncoef_plots[[3]]$g /\ncoef_plots[[4]]$g /\ncoef_plots[[5]]$g /\ncoef_plots[[6]]$g +\n  plot_layout(axes = 'collect',\n              guides = 'collect') &\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125))\n\n\n\n\n\n\n\n\nFigure 2: Estimated coefficients for basis expansions of the Motorcycle data set, with \\(K \\in \\{5,10,15,20,25,30\\}.\\)\n\n\n\n\n\nIt is apparent in Figure 2, particularly for larger \\(K\\), that some coefficients are very close to zero (though they are not zero). Particularly, this occurs to coefficients corresponding to basis functions which model the start of the mean function (around 15ms after impact), and a few of those which model the latter half of the mean function (around 40ms after impact). This yields mean function estimates close to zero (as can be seen in Figure 1)3. Though the coefficient estimates may be small, and likewise do not possess statistical or practical significance, they are nevertheless not zero (except in essentially pathological cases). As such, our model would include all \\(K\\) basis functions. A more efficient estimator would automatically identify basis functions whose significance is below a certain threshold and prune those out of the model, yielding a model with only \\(K_0 \\leq K\\) non-zero coefficients, with a comparative performance to the full model. Some procedures for automatic variable selection have been developed, which include the Least Absolute Shrinkage and Selection Operator (LASSO; see Tibshirani (1996)), the Smoothly Clipped Absolute Deviation (SCAD; see Fan and Li (2001)), the Minimax Concave Penalty (MCP; see Zhang (2010)) and Automatic Relevance Determination (ARD; see Tipping (2001)). In Section 2.1 of this vignette we illustrate the application of the ARD procedure to the Motorcyle data set.\n\n\nShow R code\nmcycle_basis_plots =\n  lapply(X = seq_along(K),\n         FUN = function(z){\n    g = \n      mcycle_fda[[z]] %&gt;%\n      {melt(data = .$design_matrixf) |&gt;\n        mutate(Var3 = unlist(lapply(X = Var2, FUN = function(x){\n          .$coefficients[x]\n        })),\n        evalargs = rep(eval_x,\n                       times = ncol(.$design_matrixf)),\n        Var2 = factor(Var2),\n        Var1 = NULL)} %&gt;%\n      ggplot(data = .,\n             mapping = aes(y = value,\n                           x = evalargs,\n                           group = Var2,\n                           col = Var3)) +\n      geom_line() +\n      scale_color_continuous(type = 'viridis',\n                             limits = c(-130, 160))\n    return(list(g = g))})\nmcycle_basis_plots[[1]]$g /\nmcycle_basis_plots[[2]]$g /\nmcycle_basis_plots[[3]]$g /\nmcycle_basis_plots[[4]]$g /\nmcycle_basis_plots[[5]]$g /\nmcycle_basis_plots[[6]]$g +\n  plot_layout(axes = 'collect',\n              guides = 'collect')\n\n\n\n\nThe Canadian weather data set\nThe Canadian weather data set (seen in J. O. Ramsay and Silverman (2002), and available in the fda package in R) consists of measurements obtained across \\(N = 35\\) Canadian cities: in particular, we focus on the daily average temperatures for each of these cities, averaged from 1960-1994 (hence, for every city we have \\(n = 365\\) temperature measurements). We denote the day of the measurement as \\(t_{ij} \\in \\{1,2,\\ldots,365\\}\\), and the daily average temperature (in Celsius) as \\(y_{ij}\\), where the subscript \\(i \\in \\{1,2,\\ldots,35\\}\\) indicates the city to which the measurement corresponds, and the subscript \\(j \\in \\{1,2,\\ldots,365\\}\\) indicates the day to which the measurement corresponds. Therefore, we denote the full data set as \\(\\{t_{ij},y_{ij}\\}^{N,n}_{i = 1,j = 1}\\)4. Similarly to the Motorcycle data set context, we denote \\(Y_{ij}\\) as the random variable corresponding to the average temperature of the \\(j\\)-th day on the \\(i\\)-th city, and assume that\n\\[\n  \\mathbb{E}[Y_{ij}] = g_i(t_{ij}) \\quad i \\in \\{1,\\ldots,N\\}, j \\in \\{1,\\ldots,n\\}.\n\\] A key difference between this formulation and that which is seen in Equation 1 is that the mean function is allowed to vary dependent on the city, that is, we assume that the mean daily temperature profile is different in each city5. Again, we’ll assume that \\(Y_{ij} \\sim \\text{Normal}(g(t_{ij}), 1/\\tau_i), \\tau_i &gt; 0\\), or equivalently\n\\[\n  y_{ij} = g_i(t_{ij}) + \\tau^{-1/2}_i \\varepsilon_{ij} \\quad i \\in \\{1,\\ldots,N\\}, j \\in \\{1,\\ldots,n\\},\n\\] where \\(\\varepsilon_{ij} \\sim \\text{Normal}(0,1), i \\in \\{1,\\ldots,N\\}, j \\in \\{1,\\ldots,n\\}\\). Again we seek to determine estimators \\(\\hat{g}_i(\\cdot)\\) for the mean temperature functions, and likewise we assume that the mean temperature functions may be decomposed as linear combinations of a set of basis functions \\(\\{\\phi_k(\\cdot)\\}_{k = 1}^K\\) of the form \\[\n  g_i(t) = \\sum^K_{k = 1}\\beta_{ik} \\phi_k(t),\n\\] for some \\(\\boldsymbol{\\beta}_i \\in \\mathbb{R}^K\\). Let \\(\\boldsymbol{\\Phi}_i \\in \\mathbb{R}^{K \\times K}\\) and \\(\\boldsymbol{\\phi}_i(t) \\in \\mathbb{R}^K\\) be defined as \\[\n  \\boldsymbol{\\Phi}_i = \\begin{pmatrix}\n    \\phi_{1}(t_{i1}) & \\phi_{2}(t_{i1}) & \\dots & \\phi_{K}(t_{i1}) \\\\\n    \\phi_{1}(t_{i2}) & \\phi_{2}(t_{i2}) & \\dots & \\phi_{K}(t_{i2}) \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\phi_{1}(t_{in}) & \\phi_{2}(t_{in}) & \\dots & \\phi_{K}(t_{in})\n  \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{\\phi}_i(t) = \\begin{pmatrix}\n    \\phi_1(t) \\\\\n    \\phi_2(t) \\\\\n    \\vdots \\\\\n    \\phi_K(t)\n  \\end{pmatrix}.\n\\] Analogously to previous results, we find that the maximum likelihood estimators of the coefficients \\(\\boldsymbol{\\beta}_i\\) are \\[\n  \\{\\hat{\\boldsymbol{\\beta}}_\\text{ML}\\}_i = (\\boldsymbol{\\Phi}_i^\\text{T}\\boldsymbol{\\Phi}_i)^{-1}\\boldsymbol{\\Phi}^\\text{T}_i\\mathrm{y}_i \\quad i \\in \\{1,\\ldots,N\\},\n\\] assuming \\(\\boldsymbol{\\Phi}_i^\\text{T} \\boldsymbol{\\Phi}_i\\) is positive-definite for \\(i \\in \\{1,\\ldots,N\\}\\). We again adopt the cubic B-spline framework for the set of basis functions, utilizing equidistant knots (starting at \\(1\\) and ending at \\(365\\)), such that the corresponding basis function set is composed of \\(K = 30\\) basis functions. The estimated mean functions, as well as the original observed data, are plotted in Figure 3.\n\n\nShow R code\nK_canadian = 30\neval_x_canadian = seq(from = 1, to = 365, length.out = 1e3 + 1)\ncanadianWeather_fda =\n  lapply(X = seq_along(CanadianWeather$place),\n         FUN = function(x){\n           design_basis = create.bspline.basis(rangeval = c(1, 365),\n                                               norder = 4,\n                                               nbasis = K_canadian)\n           design_matrix = design_basis %&gt;%\n             getbasismatrix(evalarg = 1:365,\n                            basisobj = .,\n                            nderiv = 0)\n           design_matrixf = design_basis %&gt;%\n             getbasismatrix(evalarg = eval_x_canadian,\n                            basisobj = .,\n                            nderiv = 0)\n           coefficients = design_matrix %&gt;%\n             {solve(a = t(.) %*% .,\n                    b = t(.) %*% CanadianWeather$dailyAv[ , x , 1])}\n           colnames(design_matrix) = 1:K_canadian\n           colnames(design_matrixf) = 1:K_canadian\n           return(list(K = K_canadian,\n                       design_matrix = design_matrix,\n                       design_matrixf = design_matrixf,\n                       coefficients = coefficients,\n                       fitted_values = design_matrix %*% coefficients,\n                       eval_values = design_matrixf %*% coefficients,\n                       place = CanadianWeather$place[x],\n                       day = 1:365))\n           })\n(lapply(X = seq_along(CanadianWeather$place),\n       FUN = function(x){\n         data.frame(values = canadianWeather_fda[[x]]$eval_values,\n                    eval_x = eval_x_canadian,\n                    x = factor(CanadianWeather$place[x],\n                               levels = unique(CanadianWeather$place)))\n       }) %&gt;% {do.call(what = rbind,\n                       args = .)} %&gt;%\n  ggplot(data = .,\n         mapping = aes(x = eval_x,\n                       y = values,\n                       group = x,\n                       col = x)) +\n  geom_line() +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125),\n        legend.position = \"none\") +\n  labs(x = \"Day of the year\",\n       y = substitute(paste(\"Temperature (in \", italic(\"°C\"), \")\"))) +\n  scale_color_discrete(type = viridis::viridis(n = 35)) +\n    scale_x_continuous(breaks = c(1, 100, 200, 300, 365)) +\n    geom_hline(yintercept = 0,\n                linetype = \"dashed\",\n                col = \"red\")) +\n  (lapply(X = seq_along(CanadianWeather$place),\n       FUN = function(x){\n         data.frame(values = CanadianWeather$dailyAv[ , x , 1],\n                    eval_x = 1:365,\n                    x = factor(CanadianWeather$place[x],\n                               levels = unique(CanadianWeather$place)))\n       }) %&gt;% {do.call(what = rbind,\n                       args = .)} %&gt;%\n  geom_line(inherit.aes = F,\n            data = .,\n            mapping = aes(x = eval_x,\n                          y = values,\n                          group = x,\n                          col = x)))\n\n\n\n\n\n\n\n\nFigure 3: Estimated mean functions for each city in the Canadian weather data set, with \\(K = 30\\). The original observed data are likewise plotted.\n\n\n\n\n\nNote that, unlike the Motorcycle data set, the observed functions do not clutter around \\(y = 0\\) as significantly, hence we do not necessarily believe the underlying representation of the data to be much sparser than that which is obtained using \\(K = 30\\) basis functions. Nevertheless, we believe this application may be of interest should it reveal a common structure in the way in which the sparseness manifests for the observed functions.\n\n\nSimulated data\nIn addition to the two data sets presented above, we also consider two distinct scenarios of simulated data. Firstly, we consider data comprised of \\(N\\) functions sampled \\(n\\) times each, generated from a cubic B-spline basis set of functions, with equidistant knots starting at \\(0\\) and ending at \\(1\\), such that we have 10 basis functions, however only 6 have non-zero true coefficients. The sampling design is regular (see the footnotes) in the interval \\([0,1]\\). Out goal with this simulation is to determine the accuracy of the estimated coefficients for the ARD procedure. The data is generated as follows\n\\[\n  y_{ij} = \\boldsymbol{\\phi}^\\text{T}(t_{ij})\\begin{pmatrix}\n    -2 \\\\\n    0 \\\\\n    3/2 \\\\\n    3/2 \\\\\n    0 \\\\\n    -1 \\\\\n    -1/2 \\\\\n    -1 \\\\\n    0 \\\\\n    0\n  \\end{pmatrix} + \\tau^{-1/2}\\varepsilon_{ij},\n\\tag{5}\\] where \\(\\boldsymbol{\\phi}(t)\\) is as in Equation 4 and \\(\\tau = 25\\).\n\n\nShow R code\nset.seed(23121998)\nreplication_1 = 100\nN_1 = 100\nn_1 = 100\nt_1 = seq(from = 0, to = 1, length.out = n_1)\ntau_1 = .2 ** -2\nK_1 = 10\nbeta_1 = c(-2, 0, 3/2, 3/2, 0, -1, -1/2, -1, 0, 0)\nsim_data_1 = lapply(X = 1:N_1,\n                    FUN = function(x){\n                      design_basis = create.bspline.basis(rangeval = c(0, 1),\n                                               norder = 4,\n                                               nbasis = K_1)\n                      design_matrix = design_basis %&gt;%\n                        getbasismatrix(evalarg = t_1,\n                                       basisobj = .,\n                                       nderiv = 0)\n                      y = design_matrix %*% beta_1 + (tau_1 ** (-1/2)) * rnorm(n = n_1)\n                      return(y)\n                    })\n(lapply(X = 1:N_1,\n        FUN = function(x){\n          data.frame(x = t_1,\n                     y = sim_data_1[[x]],\n                     ind = factor(x, levels = 1:N_1))\n        })) %&gt;% do.call(what = rbind,\n                        args = .) |&gt;\n  ggplot(mapping = aes(x = x,\n                       y = y,\n                       group = ind,\n                       col = ind)) +\n  geom_line() +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125),\n        legend.position = \"none\") +\n  labs(x = substitute(paste(italic(t)[ij])),\n       y = substitute(paste(italic(y)[ij]))) +\n  scale_color_discrete(type = viridis::viridis(n = N_1)) +\n    scale_x_continuous(breaks = c(0, .5, 1),\n                       labels = c(\"0\", \"1/2\", \"1\")) +\n    geom_hline(yintercept = 0,\n                linetype = \"dashed\",\n                col = \"red\")\n\n\n\n\n\n\n\n\nFigure 4: \\(N = 100\\) functions sampled at \\(n = 100\\) instants according to Equation 5\n\n\n\n\n\nFor the second simulation, we generated functional data whose mean curve is the sum of two trigonometric functions.\n\\[\n  y_{ij} = \\cos(t_{ij}) + \\sin(2 t_{ij}) + \\tau^{-1/2}\\varepsilon_{ij}\n\\tag{6}\\]\n\n\nShow R code\nset.seed(12231998)\nreplication_2 = 100\nN_2 = 100\nn_2 = 100\nt_2 = seq(from = 0, to = 2 * pi, length.out = n_2)\ntau_2 = .2 ** -2\nsim_data_2 = lapply(X = 1:N_2,\n                    FUN = function(x){\n                      y = cos(t_2) + sin(2 * t_2) + (tau_2 ** (-1/2)) * rnorm(n = n_2)\n                      return(y)\n                    })\n(lapply(X = 1:N_2,\n        FUN = function(x){\n          data.frame(x = t_2,\n                     y = sim_data_2[[x]],\n                     ind = factor(x, levels = 1:N_2))\n        })) %&gt;% do.call(what = rbind,\n                        args = .) |&gt;\n  ggplot(mapping = aes(x = x,\n                       y = y,\n                       group = ind,\n                       col = ind)) +\n  geom_line() +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125),\n        legend.position = \"none\") +\n  labs(x = substitute(paste(italic(t)[ij])),\n       y = substitute(paste(italic(y)[ij]))) +\n  scale_color_discrete(type = viridis::viridis(n = N_1)) +\n    scale_x_continuous(breaks = c(0, pi, 2 * pi),\n                       labels = expression(\"0\", pi, paste(\"2\", pi))) +\n    geom_hline(yintercept = 0,\n                linetype = \"dashed\",\n                col = \"red\")\n\n\n\n\n\n\n\n\nFigure 5: \\(N = 100\\) functions sampled at \\(n = 100\\) instants according to Equation 6"
  },
  {
    "objectID": "vignettes/ardRepresentation.html#setup",
    "href": "vignettes/ardRepresentation.html#setup",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Setup",
    "text": "Setup\nThis vignette is primarily concerned with presenting an approach to sparse representation of Functional Data utilizing the Automatic Relevance Determination (ARD) framework, itself only a topic in the wider Sparse Bayesian Learning (SBL) context. More precisely, our aim herein is to replicate some of the studies in Sousa, Souza, and Dias (2024) and Cruz, Souza, and Sousa (2024) (the former moreso than the latter, as we do not account for any correlation structure in the functions). The goal of these previous studies was to provide an adaptive Bayesian procedure to select the bases for functional data representation, the latter also extending the results of the former by including a correlation structure for the functional data, and delineating an estimation procedure utilizing the Variational Bayes (VB) approach. It did so by including a collection of Bernoulli random variables (denoted \\(Z_{k}\\)), each associated with one particular term in the basis expansion in Equation 3, resulting in the form \\(\\tilde{\\beta}_{k} = Z_{k} \\beta_{k}\\), such that \\(\\mathbb{P}(\\tilde{\\beta}_{k} = 0) &gt; 0\\). The ARD appraoch, which is presented subsequently, similarly works as Bayesian model.\n\nAutomatic Relevance Determination\nThe ARD approach attributes the following hierarchical Bayesian model:\n\\[\n  \\begin{aligned}\n    y(t_{j}) \\vert \\{\\beta_{k}\\}, \\tau & \\sim \\text{Normal}\\Bigg(\\sum^K_{k = 1}\\beta_{k}\\phi_k(t_{j}), \\tau^{-1}\\Bigg) & j \\in \\{1,\\ldots,n\\} \\\\\n    {\\beta}_{k} \\vert  \\alpha_{k} & \\sim \\text{Normal}(0,\\alpha_{k}^{-1}) & k \\in \\{1,\\ldots,K\\}.\n  \\end{aligned}\n\\] Where \\(\\{\\alpha_{k}\\}\\) (greater than zero) are hyperparameters which dictate the concentration of the coefficients around zero, whilst \\(\\tau\\) (also greater than zero) is a hyperparameter which dictates the precision of the sampled observations. In adopting the empirical Bayes approach to estimate the model, we aim to determine the hyperparameters values which maximize the distribution of the observations points marginalized over the coefficients \\(\\{\\beta_{k}\\}\\). Tipping (2001) proposes a straightforward set of update equations for the hyperparameters (as well as for the coefficients, which are estimated via their maximum a posteriori value). These are as follows:\n\\[\n  \\hat{\\boldsymbol{\\beta}} = \\hat{\\tau} \\boldsymbol{\\Sigma}\\Phi^\\text{T}\\mathrm{y}\n\\tag{7}\\]\n\\[\n  \\hat{\\alpha}_k = \\frac{\\gamma_k}{\\hat{\\beta}_k^2}\n\\tag{8}\\]\n\\[\n  \\frac{1}{\\hat{\\tau}} = \\frac{\\lvert\\lvert \\textbf{y} - \\Phi \\hat{\\boldsymbol{\\beta}}\\rvert\\rvert^2}{n - \\sum^K_{k = 1}\\gamma_{k}} \\quad k \\in \\{1,\\ldots,K\\}\n\\tag{9}\\]\nwhere\n\\[\n  \\boldsymbol{\\Sigma} = (\\text{diag}(\\hat{\\alpha}) + \\hat{\\tau} \\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi})^{-1}\n\\tag{10}\\]\n\\[\n  \\gamma_{k} = 1 - \\hat{\\alpha}_{k}\\Sigma_{kk} \\quad k \\in \\{1,\\ldots,K\\}.\n\\tag{11}\\]\nTipping (2001) likewise demonstrates that, under certain circumstances, \\(\\hat{\\alpha}_k \\approx \\infty\\), under which condition the prior distribution of \\(\\beta_k \\vert \\alpha_k\\) is approximately degenerate at 0 (as is the consequent posterior). We provide an algorithm for the estimation of an ARD model, given the above updating equations:\n\nInitialization. Enter a valid response vector \\(\\mathrm{y}\\) and a valid predictor matrix \\(\\boldsymbol{\\Phi}\\). Set a cutoff value \\(M &gt; 0\\) such that, if \\(\\hat{\\alpha}_k &gt; M\\), \\(\\hat{\\beta}_k = 0\\). Set initial values for the estimators \\(\\hat{\\tau} = \\tau_0\\) and \\(\\hat{\\alpha} = \\alpha_0\\).\n\nCompute the quantities in Equation 10 and Equation 11.\nUpdate \\(\\hat{\\boldsymbol{\\beta}}\\) according to Equation 7.\nUpdate \\(\\hat{\\alpha}_k\\) and \\(\\hat{\\tau}\\) according to Equation 8 and Equation 9.\nFor every \\(k \\in \\{1,\\ldots,K\\}\\) if \\(\\hat{\\alpha}_{k} &gt; M\\), set \\(\\hat{\\beta}_{k} = 0\\) and \\(\\hat{\\alpha}_k = \\infty\\).\nCheck the convergence of the estimates. If convergence has occurred, stop. Else, return to step (1).\n\n\nThe function ard_model() in the chunk below performs the estimating procedure.\n\n\nShow R code\nard_model = function(y,\n                     Phi,\n                     tau_ini = 1,                             \n                     alpha_ini = rep(x = 1,\n                                     times = ncol(Phi)),\n                     max_iter = 1e3,\n                     tol = 1e-8,\n                     cutoff = 1e4)\n{\n  time = Sys.time()\n  n = length(y)\n  K = ncol(Phi)\n  \n  tau = tau_ini\n  alpha = alpha_ini\n  keep = !logical(K)\n  \n  beta_est = rep(0, times = K)\n  gamma_est = rep(0, times = K)\n  \n  iter = 1\n  cond = Inf\n  beta_est0 = beta_est + Inf\n  \n  while((iter &lt; max_iter) & (abs(cond) &gt; tol))\n  {\n    Phi_keep = Phi[,keep]\n    \n    A = diag(alpha[keep])\n    \n    Sigma = solve(A + tau * t(Phi_keep) %*% Phi_keep)\n    \n    beta_est[keep] = tau * Sigma %*% t(Phi_keep) %*% y\n    \n    gamma_est[keep] = 1 - alpha[keep] * diag(Sigma)\n    \n    alpha[keep] = gamma_est[keep] / (beta_est[keep] ** 2)\n    tau = (n - sum(gamma_est[keep])) / sum((y - Phi_keep %*% beta_est[keep]) ** 2)\n    \n    keep = ifelse(test = alpha &lt; cutoff, yes = T, no = F)\n    \n    beta_est[!keep] = 0\n    alpha[!keep] = Inf\n    \n    iter %&lt;&gt;% + 1\n    \n    cond = sum((beta_est - beta_est0) ** 2) \n    beta_est0 = beta_est\n  }\n  \n  time %&lt;&gt;% {Sys.time() - .}\n  \n  return(list(iter = iter,\n              alpha = alpha[keep],\n              beta = beta_est[keep],\n              gamma = gamma_est[keep],\n              keep = which(keep),\n              time = time,\n              fitted_values = Phi[,keep] %*% beta_est[keep]))\n}\n\n\nBishop (2006) suggests utilizing \\(\\{\\gamma_k\\}\\), defined in Equation 11, as a measure of the model complexity, defining the effective degrees of freedom of the model as \\[\n  p_\\text{eff} = \\sum^K_{k = 1}\\gamma_k,\n\\] where, if the \\(k\\)-th coefficient is included in the model, \\(\\gamma_k \\in [0,1]\\), whereas if is not included, \\(\\gamma_k = 0\\)."
  },
  {
    "objectID": "vignettes/ardRepresentation.html#fixed-point-updates",
    "href": "vignettes/ardRepresentation.html#fixed-point-updates",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Fixed Point Updates",
    "text": "Fixed Point Updates\nA"
  },
  {
    "objectID": "vignettes/ardRepresentation.html#expectation-maximization-approach",
    "href": "vignettes/ardRepresentation.html#expectation-maximization-approach",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Expectation Maximization Approach",
    "text": "Expectation Maximization Approach\nB"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Felipe Ferreira",
    "section": "",
    "text": "I’m a Statistician, having received my bachelor’s degree in Statistics (2022) from the Federal University of Juiz de Fora and my Master’s degree in Statistics (2024) from the University of São Paulo. My main research interests are Functional Data Analysis and Approximate Inference.\nCheck out my undergraduate thesis (in Portuguese) or my Master’s thesis."
  },
  {
    "objectID": "solutions.html",
    "href": "solutions.html",
    "title": "Exercise Solutions",
    "section": "",
    "text": "This page consists of exercise solutions of books I have been working through.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntrodução à Inferência Estatística\n\n\nHeleno Bolfarine and Mônica Carneiro Sandoval\n\n\n\n\n\n\n\n\nJul 2, 2025\n\n\nFelipe Toledo Ferreira\n\n\n\n\n\n\n\n\n\n\n\n\nPattern Recognition and Machine Learning\n\n\nChristopher M. Bishop\n\n\n\n\n\n\n\n\nJul 2, 2025\n\n\nFelipe Toledo Ferreira\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "vignettes/ardRepresentation.html#application-i",
    "href": "vignettes/ardRepresentation.html#application-i",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Application I",
    "text": "Application I\n\n\nShow R code\nind = 4\nmcycle_ard = ard_model(y = mcycle$accel,\n                       Phi = mcycle_fda[[ind]]$design_matrix,\n                       cutoff = 1e-2)\n(mcycle_ard %&gt;% {data.frame(eval_x = eval_x,\n                           eval_values = mcycle_fda[[ind]]$design_matrixf[,.$keep] %*% .$beta)} |&gt;\n  ggplot(mapping = aes(x = eval_x,\n                       y = eval_values)) +\n  geom_line(col = \"blue\") +\n  (mcycle |&gt; geom_point(inherit.aes = F,\n                        mapping = aes(x = times,\n                                      y = accel))) +\n  (mcycle_fda[[ind]] %&gt;% {data.frame(eval_x = eval_x,\n                                   eval_values = .$design_matrixf %*% .$coefficients)} |&gt;\n     geom_line(inherit.aes = F,\n               mapping = aes(x = eval_x,\n                             y = eval_values),\n               col = \"red\")) +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125)) +\n  labs(x = substitute(paste(\"Time after impact (in \", italic(\"ms\"), \")\")),\n       y = substitute(paste(\"Acceleration (in \", italic(\"g\"), \")\")))) /\n(coef_plots[[ind]]$g +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125)) +\n  (mcycle_ard %&gt;% {data.frame(x = .$keep,\n                              y = .$beta) |&gt;\n      geom_point(inherit.aes = F,\n                 mapping = aes(x = x,\n                               y = y),\n                 col = \"red\")})) + plot_layout(heights = c(2, 1))\nn = nrow(mcycle)\np = length(mcycle_ard$keep)\nR2_adj = 1 - ((n - 1) / (n - sum(mcycle_ard$gamma) - 1)) * sum((mcycle_ard$fitted_values - mcycle$accel) ** 2) / sum((mean(mcycle$accel) - mcycle$accel) ** 2)\n\n\n\n\n\n\n\n\nFigure 3: Left panel: sample estimated density of the fat percentage. Right panel: sample boxplot of the fat percentage.\n\n\n\n\n\nResulting in a \\(R^2_{\\text{adj}} \\approx 0.7847\\)"
  },
  {
    "objectID": "vignettes/ardRepresentation.html#application-ii",
    "href": "vignettes/ardRepresentation.html#application-ii",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Application II",
    "text": "Application II\n\n\nShow R code\ncanadianWeather_ard = lapply(X = seq_along(CanadianWeather$place),\n                             FUN = function(x){\n                               ard_model(y = CanadianWeather$dailyAv[ , x, 1],\n                                         Phi = canadianWeather_fda[[x]]$design_matrix,\n                                         cutoff = 1e1)\n                             })\n(canadianWeather_ard %&gt;% {lapply(X = seq_along(CanadianWeather$place),\n        FUN = function(x){\n          data.frame(eval_x = eval_x_canadian,\n                     values = canadianWeather_fda[[x]]$design_matrixf[,.[[x]]$keep] %*% .[[x]]$beta,\n                     x = factor(CanadianWeather$place[x],\n                               levels = unique(CanadianWeather$place)))\n        })} %&gt;% {do.call(what = rbind,\n                       args = .)} %&gt;%\n  ggplot(data = .,\n         mapping = aes(x = eval_x,\n                       y = values,\n                       group = x,\n                       col = x)) +\n    geom_line() +\n    theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125),\n        legend.position = \"none\") +\n  labs(x = \"Day of the year\",\n       y = substitute(paste(\"Temperature (in \", italic(\"°C\"), \")\"))) +\n  scale_color_discrete(type = viridis::viridis(n = 35)) +\n    scale_x_continuous(breaks = c(1, 100, 200, 300, 365)) +\n    geom_hline(yintercept = 0,\n                linetype = \"dashed\",\n                col = \"red\")) +\n  (lapply(X = seq_along(CanadianWeather$place),\n       FUN = function(x){\n         data.frame(values = CanadianWeather$dailyAv[ , x , 1],\n                    eval_x = 1:365,\n                    x = factor(CanadianWeather$place[x],\n                               levels = unique(CanadianWeather$place)))\n       }) %&gt;% {do.call(what = rbind,\n                       args = .)} %&gt;%\n  geom_line(inherit.aes = F,\n            data = .,\n            mapping = aes(x = eval_x,\n                          y = values,\n                          group = x,\n                          col = x)))\ntime_canadian_ARD = canadianWeather_ard %&gt;% {lapply(X = seq_along(CanadianWeather$place), FUN = function(x){.[[x]]$time})} |&gt; unlist() |&gt; sum()\n\n\n\n\n\n\n\n\nFigure 7: Estimated mean functions for each city in the Canadian weather data set, with \\(K = 30\\). The original observed data are likewise plotted.\n\n\n\n\n\nNote that the ARD estimation procedure for all functions took approximately 0.2905 seconds."
  },
  {
    "objectID": "vignettes/ardRepresentation.html#simulation-study-i",
    "href": "vignettes/ardRepresentation.html#simulation-study-i",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Simulation Study I",
    "text": "Simulation Study I"
  },
  {
    "objectID": "vignettes/ardRepresentation.html#simulation-study-ii",
    "href": "vignettes/ardRepresentation.html#simulation-study-ii",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Simulation Study II",
    "text": "Simulation Study II"
  },
  {
    "objectID": "vignettes/ardRepresentation.html#simulation-study-iii",
    "href": "vignettes/ardRepresentation.html#simulation-study-iii",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Simulation Study III",
    "text": "Simulation Study III"
  },
  {
    "objectID": "vignettes/ardRepresentation.html#footnotes",
    "href": "vignettes/ardRepresentation.html#footnotes",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe avoid denoting the accelerometer measurements as \\(y(t_j)\\) since, as stated, there are measurement times \\(t_j\\) which appear repeatedly, with distinct \\(y_j\\), such that writing \\(y(t_j) = y_j\\) would result in inconsistencies. The same is true for the random variables from which these observations are drawn, which we denote as \\(Y_j\\).↩︎\nThe equivalency between Equation 1 and Equation 2 is a consequence of the assumption of normality for the random errors in Equation 2 (although this equivalency would hold under certain other distributions). The form in Equation 2 is more appealing due to its inherent distinction between the deterministic and stochastic components, however, the formulation in Equation 1 (coupled with distributional assumptions on \\(Y_j\\)) is adopted under the generic framework of ‘generalized linear models’ (or particularly, for function estimation, the framework of ‘generalized additive models’; see Hastie and Tibshirani (1986)).↩︎\nThis behavior is expected: as the object is approximately inert prior to the impact, likewise, in the immediate milliseconds following impact, it is approximately inert, eventually gaining acceleration, which dissipates as time passes, and the object eventually returns to inertia.↩︎\nUnlike the Motorcycle data set, for any fixed city \\(i\\) in the Canadian weather data set, the observation instants do not repeat and are all equally spaced. Also importantly, they are the same across all cities (that is, \\(t_{ij} = t_j, \\forall i \\in \\{1,\\ldots,N\\}\\)). This likewise implies that the matrix \\(\\boldsymbol{\\Phi}_i\\) defined later is the same independent of the subscript \\(i\\). We opt to not remove the subscript in this data set to preserve the generality of the content. When the observation instants satisfy these conditions, the sampling design is said to be regular.↩︎\nHowever, it’s expected that some mean temperature functions present roughly similar behavior due to geographical proximity and climate similarity of the corresponding cities. These similarities have been studied in applications in functional data clustering, as seen in Xian et al. (2024).↩︎"
  },
  {
    "objectID": "vignettes/ardRepresentation.html#sec-app-mcycle",
    "href": "vignettes/ardRepresentation.html#sec-app-mcycle",
    "title": "Automatic Relevance Determination for Functional Data Representation",
    "section": "Application I",
    "text": "Application I\n\n\nShow R code\nind = 4\nmcycle_ard = ard_model(y = mcycle$accel,\n                       Phi = mcycle_fda[[ind]]$design_matrix,\n                       cutoff = 1e-2)\n(mcycle_ard %&gt;% {data.frame(eval_x = eval_x,\n                           eval_values = mcycle_fda[[ind]]$design_matrixf[,.$keep] %*% .$beta)} |&gt;\n  ggplot(mapping = aes(x = eval_x,\n                       y = eval_values)) +\n  geom_line(col = \"blue\") +\n  (mcycle |&gt; geom_point(inherit.aes = F,\n                        mapping = aes(x = times,\n                                      y = accel))) +\n  (mcycle_fda[[ind]] %&gt;% {data.frame(eval_x = eval_x,\n                                   eval_values = .$design_matrixf %*% .$coefficients)} |&gt;\n     geom_line(inherit.aes = F,\n               mapping = aes(x = eval_x,\n                             y = eval_values),\n               col = \"red\")) +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125)) +\n  labs(x = substitute(paste(\"Time after impact (in \", italic(\"ms\"), \")\")),\n       y = substitute(paste(\"Acceleration (in \", italic(\"g\"), \")\"))) +\n    geom_hline(col = \"red\",\n               linetype = \"dashed\",\n               yintercept = 0)) /\n(coef_plots[[ind]]$g +\n  theme_classic() +\n  theme(text = element_text(size = unit(11, \"pt\"),\n                            family = \"LM Roman 10\",\n                            color = \"black\"),\n        axis.line = element_line(arrow = grid::arrow(length = unit(0.08, \"cm\"), \n                                                     ends = \"last\",\n                                                     angle = 30,\n                                                     type = \"closed\"),\n                                 linewidth = .25),\n        axis.ticks = element_line(color = \"black\",\n                                  linewidth = .125)) +\n  (mcycle_ard %&gt;% {data.frame(x = .$keep,\n                              y = .$beta) |&gt;\n      geom_point(inherit.aes = F,\n                 mapping = aes(x = x,\n                               y = y),\n                 col = \"red\")})) + plot_layout(heights = c(2, 1))\nn = nrow(mcycle)\np = length(mcycle_ard$keep)\nR2_adj = 1 - ((n - 1) / (n - sum(mcycle_ard$gamma))) * sum((mcycle_ard$fitted_values - mcycle$accel) ** 2) / sum((mean(mcycle$accel) - mcycle$accel) ** 2)\n\n\n\n\n\n\n\n\nFigure 6: Left panel: sample estimated density of the fat percentage. Right panel: sample boxplot of the fat percentage.\n\n\n\n\n\nResulting in \\(R^2_{\\text{adj}} \\approx 0.7864\\)"
  }
]